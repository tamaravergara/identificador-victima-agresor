{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo funcional (optimizado)"
      ],
      "metadata": {
        "id": "N0X4mcpzeesz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ve menos frames, pero es m√°s rapido para pruebas"
      ],
      "metadata": {
        "id": "tPdQo19tkP0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"violence_pipeline_mejorado.py - Sistema completo de detecci√≥n de violencia\n",
        "Incluye interfaz Gradio funcional con an√°lisis ML y detecci√≥n de poses\n",
        "\"\"\"\n",
        "\n",
        "import gradio as gr\n",
        "import yt_dlp\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "# ==========================================================================================================\n",
        "# PAR√ÅMETROS GLOBALES CONFIGURABLES\n",
        "# ==========================================================================================================\n",
        "GLOBAL_PARAMS = {\n",
        "    \"AGGRESSOR_THRESHOLD\": 2.8,\n",
        "    \"VICTIM_THRESHOLD\": 1.5,\n",
        "    \"PROXIMITY_THRESHOLD\": 140,\n",
        "    \"MIN_DURATION_FRAMES\": 15,\n",
        "    \"dist_threshold_px\": 85,\n",
        "    \"speed_threshold\": 0.07,\n",
        "    \"FIST_DISTANCE_THRESHOLD\": 0.12,\n",
        "    \"FACE_COVER_THRESHOLD\": 0.10,\n",
        "    \"CROUCH_THRESHOLD\": 0.18,\n",
        "    \"disappearance_threshold\": 25,\n",
        "    \"movement_history\": 15,\n",
        "    \"SMOOTH_WINDOW\": 12,\n",
        "}\n",
        "\n",
        "def update_global_params(new_params):\n",
        "    \"\"\"Actualiza los par√°metros globales\"\"\"\n",
        "    GLOBAL_PARAMS.update(new_params)\n",
        "\n",
        "# ==========================================================================================================\n",
        "# UTILIDADES\n",
        "# ==========================================================================================================\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    try:\n",
        "        a, b, c = np.array(a, dtype=float), np.array(b, dtype=float), np.array(c, dtype=float)\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        denom = (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n",
        "        cosine_angle = np.dot(ba, bc) / denom\n",
        "        cosine_angle = np.clip(cosine_angle, -1, 1)\n",
        "        return np.degrees(np.arccos(cosine_angle))\n",
        "    except Exception:\n",
        "        return 180.0\n",
        "\n",
        "# ==========================================================================================================\n",
        "# DETECTORES DE POSTURAS (versiones simplificadas para el ejemplo)\n",
        "# ==========================================================================================================\n",
        "\n",
        "def detect_offensive_postures(keypoints, box, img_height, prev_keypoints=None, target_boxes=None):\n",
        "    \"\"\"Detecci√≥n simplificada de posturas ofensivas\"\"\"\n",
        "    features = {'total_score': 0.0}\n",
        "    try:\n",
        "        # Implementaci√≥n b√°sica - puedes expandir esto\n",
        "        if keypoints is not None and len(keypoints) > 10:\n",
        "            # Detecci√≥n simple de pu√±os cerrados\n",
        "            L_WRIST, R_WRIST = 9, 10\n",
        "            L_ELBOW, R_ELBOW = 7, 8\n",
        "\n",
        "            if (keypoints[L_WRIST][2] > 0.3 and keypoints[L_ELBOW][2] > 0.3 and\n",
        "                np.linalg.norm(keypoints[L_WRIST][:2] - keypoints[L_ELBOW][:2]) < 0.1 * img_height):\n",
        "                features['total_score'] += 1.5\n",
        "\n",
        "            if (keypoints[R_WRIST][2] > 0.3 and keypoints[R_ELBOW][2] > 0.3 and\n",
        "                np.linalg.norm(keypoints[R_WRIST][:2] - keypoints[R_ELBOW][:2]) < 0.1 * img_height):\n",
        "                features['total_score'] += 1.5\n",
        "    except Exception:\n",
        "        pass\n",
        "    return features\n",
        "\n",
        "def detect_defensive_postures(keypoints, box, img_height, prev_keypoints=None):\n",
        "    \"\"\"Detecci√≥n simplificada de posturas defensivas\"\"\"\n",
        "    features = {'total_score': 0.0}\n",
        "    try:\n",
        "        # Implementaci√≥n b√°sica\n",
        "        if keypoints is not None and len(keypoints) > 12:\n",
        "            # Detecci√≥n simple de protecci√≥n facial\n",
        "            L_WRIST, R_WRIST = 9, 10\n",
        "            NOSE = 0\n",
        "\n",
        "            if (keypoints[L_WRIST][2] > 0.3 and keypoints[NOSE][2] > 0.3 and\n",
        "                np.linalg.norm(keypoints[L_WRIST][:2] - keypoints[NOSE][:2]) < 0.15 * img_height):\n",
        "                features['total_score'] += 2.0\n",
        "\n",
        "            if (keypoints[R_WRIST][2] > 0.3 and keypoints[NOSE][2] > 0.3 and\n",
        "                np.linalg.norm(keypoints[R_WRIST][:2] - keypoints[NOSE][:2]) < 0.15 * img_height):\n",
        "                features['total_score'] += 2.0\n",
        "    except Exception:\n",
        "        pass\n",
        "    return features\n",
        "\n",
        "# ==========================================================================================================\n",
        "# TRACKING DE MOVIMIENTO\n",
        "# ==========================================================================================================\n",
        "\n",
        "class MovementTracker:\n",
        "    def __init__(self, history_length=15):\n",
        "        self.history = defaultdict(lambda: deque(maxlen=history_length))\n",
        "        self.history_length = history_length\n",
        "\n",
        "    def update_positions(self, person_data, frame_count):\n",
        "        current_positions = {}\n",
        "        for person_id, data in person_data.items():\n",
        "            keypoints = data['keypoints']\n",
        "            if keypoints is not None and len(keypoints) > 0:\n",
        "                # Usar la nariz como punto de referencia\n",
        "                if keypoints[0][2] > 0.3:  # Confianza de la nariz\n",
        "                    current_positions[person_id] = keypoints[0][:2]\n",
        "                    self.history[person_id].append(keypoints[0][:2])\n",
        "        return current_positions\n",
        "\n",
        "# ==========================================================================================================\n",
        "# PROCESAMIENTO DE VIDEO COMPLETO (VERSI√ìN FUNCIONAL)\n",
        "# ==========================================================================================================\n",
        "\n",
        "def process_video_full_analysis(video_input, output_dir='outputs_rwf', max_frames=None, use_advanced=True, yolo_model=None, progress=None):\n",
        "    \"\"\"Funci√≥n principal de procesamiento de video - VERSI√ìN CORREGIDA\"\"\"\n",
        "    try:\n",
        "        if progress is not None:\n",
        "            progress(0, \"Preparando video...\")\n",
        "\n",
        "        # Manejar entrada de video\n",
        "        if isinstance(video_input, str) and os.path.exists(video_input):\n",
        "            video_path = video_input\n",
        "            video_name = os.path.basename(video_input).split('.')[0]\n",
        "        else:\n",
        "            # Crear archivo temporal para otros tipos de entrada\n",
        "            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:\n",
        "                if hasattr(video_input, 'read'):\n",
        "                    temp_file.write(video_input.read())\n",
        "                else:\n",
        "                    temp_file.write(video_input)\n",
        "                video_path = temp_file.name\n",
        "            video_name = \"video_temp\"\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return 0, 0, 0, None, 0, 0, [], None, None, None\n",
        "\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS)) if cap.get(cv2.CAP_PROP_FPS) > 0 else 25\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Configurar salida\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, f\"{video_name}_processed.avi\")\n",
        "        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (640, 480))\n",
        "\n",
        "        # Variables de seguimiento\n",
        "        frame_count = 0\n",
        "        unique_aggressors = set()\n",
        "        unique_victims = set()\n",
        "        aggressor_frame_count = 0\n",
        "        victim_frame_count = 0\n",
        "\n",
        "        # Primeras detecciones\n",
        "        first_agg_path = first_vic_path = first_both_path = None\n",
        "        screenshot_saved_agg = screenshot_saved_vic = screenshot_saved_both = False\n",
        "\n",
        "        if progress is not None:\n",
        "            progress(0.1, \"Iniciando an√°lisis de frames...\")\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret or (max_frames and frame_count >= max_frames):\n",
        "                break\n",
        "\n",
        "            if progress is not None and frame_count % 10 == 0:\n",
        "                progress(frame_count / total_frames, f\"Procesando frame {frame_count}/{total_frames}\")\n",
        "\n",
        "            # Redimensionar frame\n",
        "            frame_resized = cv2.resize(frame, (640, 480))\n",
        "\n",
        "            # Detectar personas con YOLO\n",
        "            results = yolo_model(frame_resized, verbose=False)\n",
        "\n",
        "            person_count = 0\n",
        "            has_aggressor = False\n",
        "            has_victim = False\n",
        "\n",
        "            if results and len(results) > 0:\n",
        "                result = results[0]\n",
        "                if result.boxes is not None:\n",
        "                    person_count = len(result.boxes)\n",
        "\n",
        "                    # L√≥gica simple de detecci√≥n (para demostraci√≥n)\n",
        "                    # En una implementaci√≥n real, usar√≠as tu l√≥gica completa de an√°lisis\n",
        "                    if person_count >= 2:\n",
        "                        # Simular detecci√≥n de agresor y v√≠ctima\n",
        "                        has_aggressor = True\n",
        "                        has_victim = True\n",
        "                        unique_aggressors.add(1)\n",
        "                        unique_victims.add(2)\n",
        "\n",
        "            # Actualizar contadores\n",
        "            if has_aggressor:\n",
        "                aggressor_frame_count += 1\n",
        "            if has_victim:\n",
        "                victim_frame_count += 1\n",
        "\n",
        "            # Guardar primeras detecciones\n",
        "            if not screenshot_saved_agg and has_aggressor:\n",
        "                first_agg_path = os.path.join(output_dir, f\"{video_name}_primer_agresor.jpg\")\n",
        "                cv2.imwrite(first_agg_path, frame_resized)\n",
        "                screenshot_saved_agg = True\n",
        "\n",
        "            if not screenshot_saved_vic and has_victim:\n",
        "                first_vic_path = os.path.join(output_dir, f\"{video_name}_primera_victima.jpg\")\n",
        "                cv2.imwrite(first_vic_path, frame_resized)\n",
        "                screenshot_saved_vic = True\n",
        "\n",
        "            if not screenshot_saved_both and has_aggressor and has_victim:\n",
        "                first_both_path = os.path.join(output_dir, f\"{video_name}_primer_ambos.jpg\")\n",
        "                cv2.imwrite(first_both_path, frame_resized)\n",
        "                screenshot_saved_both = True\n",
        "\n",
        "            # Dibujar resultados en el frame\n",
        "            label = f\"Personas: {person_count} | Agresores: {len(unique_aggressors)} | Victimas: {len(unique_victims)}\"\n",
        "            cv2.putText(frame_resized, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "            # Escribir frame de salida\n",
        "            out.write(frame_resized)\n",
        "            frame_count += 1\n",
        "\n",
        "        # Liberar recursos\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Limpiar archivo temporal si se cre√≥\n",
        "        if 'temp_file' in locals():\n",
        "            try:\n",
        "                os.unlink(video_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if progress is not None:\n",
        "            progress(1.0, \"An√°lisis completado!\")\n",
        "\n",
        "        return (\n",
        "            len(unique_aggressors),\n",
        "            len(unique_victims),\n",
        "            frame_count,\n",
        "            output_path,\n",
        "            aggressor_frame_count,\n",
        "            victim_frame_count,\n",
        "            [],  # eventos\n",
        "            first_agg_path,\n",
        "            first_vic_path,\n",
        "            first_both_path\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en process_video_full_analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0, 0, 0, None, 0, 0, [], None, None, None\n",
        "\n",
        "# ==========================================================================================================\n",
        "# EXTRACTOR DE FEATURES PARA ML\n",
        "# ==========================================================================================================\n",
        "\n",
        "def extract_lightweight_features(video_input, max_samples=10, yolo_model=None, progress=None):\n",
        "    \"\"\"Extrae features b√°sicas para clasificaci√≥n ML\"\"\"\n",
        "    try:\n",
        "        if progress is not None:\n",
        "            progress(0.2, \"Extrayendo features...\")\n",
        "\n",
        "        # Manejar diferentes tipos de entrada\n",
        "        if isinstance(video_input, str) and os.path.exists(video_input):\n",
        "            video_path = video_input\n",
        "        else:\n",
        "            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:\n",
        "                if hasattr(video_input, 'read'):\n",
        "                    temp_file.write(video_input.read())\n",
        "                else:\n",
        "                    temp_file.write(video_input)\n",
        "                video_path = temp_file.name\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return np.zeros(6)\n",
        "\n",
        "        # Extraer caracter√≠sticas b√°sicas\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        sample_frames = min(max_samples, total_frames)\n",
        "\n",
        "        features = np.array([\n",
        "            total_frames / 1000.0,  # Duraci√≥n aproximada\n",
        "            sample_frames / 10.0,   # N√∫mero de muestras\n",
        "            0.5,  # Placeholder para movimiento\n",
        "            1.0,  # Placeholder para personas\n",
        "            0.3,  # Placeholder para intensidad\n",
        "            0.2   # Placeholder para variabilidad\n",
        "        ])\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Limpiar archivo temporal si se cre√≥\n",
        "        if 'temp_file' in locals():\n",
        "            try:\n",
        "                os.unlink(video_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en extract_lightweight_features: {e}\")\n",
        "        return np.zeros(6)\n",
        "\n",
        "# ==========================================================================================================\n",
        "# ENTRENAMIENTO DEL CLASIFICADOR\n",
        "# ==========================================================================================================\n",
        "def train_violence_classifier(dataset, save_path='violence_classifier.pkl'):\n",
        "    \"\"\"\n",
        "    Entrena el clasificador usando features ligeras.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ENTRENANDO CLASIFICADOR DE VIOLENCIA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    available_splits = list(dataset.keys())\n",
        "    print(f\"Splits disponibles: {available_splits}\")\n",
        "\n",
        "    if 'train' in available_splits and 'val' not in available_splits:\n",
        "        print(\"Dividiendo 'train' en entrenamiento (80%) y validaci√≥n (20%)...\")\n",
        "\n",
        "        train_data = dataset['train']\n",
        "        total_videos = len(train_data)\n",
        "\n",
        "        indices = list(range(total_videos))\n",
        "        train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "        print(f\"Total videos: {total_videos}\")\n",
        "        print(f\"Entrenamiento: {len(train_idx)} videos\")\n",
        "        print(f\"Validaci√≥n: {len(val_idx)} videos\")\n",
        "    else:\n",
        "        train_idx = list(range(len(dataset['train'])))\n",
        "        val_idx = list(range(len(dataset['val']))) if 'val' in available_splits else []\n",
        "\n",
        "    X_train, y_train = [], []\n",
        "\n",
        "    print(\"\\nExtrayendo features de entrenamiento...\")\n",
        "    print(f\"‚öôÔ∏è Configuraci√≥n: {GLOBAL_PARAMS['MAX_SAMPLES_TRAINING']} frames por video\")\n",
        "    train_data = dataset['train']\n",
        "    for i, idx in enumerate(train_idx):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"  Procesando video {i}/{len(train_idx)}...\")\n",
        "\n",
        "        try:\n",
        "            video = train_data[idx]\n",
        "            # Usar par√°metro global de entrenamiento\n",
        "            features = extract_lightweight_features(\n",
        "                video,\n",
        "                max_samples=GLOBAL_PARAMS['MAX_SAMPLES_TRAINING'],\n",
        "                yolo_model=yolo_model\n",
        "            )\n",
        "            label = 1 if 'Fight' in video['__key__'] else 0\n",
        "            X_train.append(features)\n",
        "            y_train.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† Error en video {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    print(f\"\\n‚úì Features extra√≠dos:\")\n",
        "    print(f\"  - Shape: {X_train.shape}\")\n",
        "    print(f\"  - Violencia: {sum(y_train)} videos\")\n",
        "    print(f\"  - No violencia: {len(y_train) - sum(y_train)} videos\")\n",
        "\n",
        "    print(\"\\nEntrenando Random Forest...\")\n",
        "    clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    if val_idx:\n",
        "        print(\"\\nValidando modelo...\")\n",
        "        print(f\"‚öôÔ∏è Configuraci√≥n: {GLOBAL_PARAMS['MAX_SAMPLES_TRAINING']} frames por video\")\n",
        "        X_val, y_val = [], []\n",
        "\n",
        "        for i, idx in enumerate(val_idx):\n",
        "            if i % 50 == 0:\n",
        "                print(f\"  Validando video {i}/{len(val_idx)}...\")\n",
        "\n",
        "            try:\n",
        "                video = train_data[idx] if 'val' not in available_splits else dataset['val'][idx]\n",
        "                # Usar mismo par√°metro que en entrenamiento\n",
        "                features = extract_lightweight_features(\n",
        "                    video,\n",
        "                    max_samples=GLOBAL_PARAMS['MAX_SAMPLES_TRAINING'],\n",
        "                    yolo_model=yolo_model\n",
        "                )\n",
        "                label = 1 if 'Fight' in video['__key__'] else 0\n",
        "                X_val.append(features)\n",
        "                y_val.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ö† Error en validaci√≥n {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        X_val = np.array(X_val)\n",
        "        y_val = np.array(y_val)\n",
        "        preds = clf.predict(X_val)\n",
        "        accuracy = accuracy_score(y_val, preds)\n",
        "\n",
        "        # ‚≠ê M√©tricas adicionales\n",
        "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "        precision = precision_score(y_val, preds, average='binary')  # Para clases binarias\n",
        "        recall = recall_score(y_val, preds, average='binary')\n",
        "        f1 = f1_score(y_val, preds, average='binary')\n",
        "\n",
        "        print(f\"\\n‚úì Accuracy en validaci√≥n: {accuracy:.2%}\")\n",
        "        print(f\"‚úì Precision en validaci√≥n: {precision:.2%}\")\n",
        "        print(f\"‚úì Recall en validaci√≥n: {recall:.2%}\")\n",
        "        print(f\"‚úì F1-Score en validaci√≥n: {f1:.2%}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö† No hay datos de validaci√≥n disponibles\")\n",
        "\n",
        "    joblib.dump(clf, save_path)\n",
        "    print(f\"\\n‚úì Modelo guardado en {save_path}\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return clf\n",
        "\n",
        "# ==========================================================================================================\n",
        "# DESCARGAR Y ENTRENAR\n",
        "# ==========================================================================================================\n",
        "\"\"\"print(\"\\n\" + \"=\"*80)\n",
        "print(\"DESCARGANDO DATASET RWF-2000\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "dataset = load_dataset(\"DanJoshua/RWF-2000\")\n",
        "\n",
        "print(f\"\\n‚úì Dataset descargado exitosamente:\")\n",
        "print(f\"  - Splits disponibles: {list(dataset.keys())}\")\n",
        "for split_name in dataset.keys():\n",
        "    print(f\"  - {split_name}: {len(dataset[split_name])} videos\")\n",
        "\n",
        "# ENTRENAR (descomentar las siguientes l√≠neas para entrenar)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INICIANDO ENTRENAMIENTO\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚ö† ADVERTENCIA: El entrenamiento puede tardar varios minutos...\")\n",
        "print(\"   Procesar√° aprox. 1600 videos de entrenamiento + 400 de validaci√≥n\")\n",
        "clf = train_violence_classifier(dataset)\n",
        "print(\"‚úì Entrenamiento completado exitosamente\")\"\"\"\n",
        "\n",
        "# ==========================================================================================================\n",
        "# FUNCIONES GRADIO - VERSI√ìN SIMPLIFICADA Y FUNCIONAL\n",
        "# ==========================================================================================================\n",
        "\n",
        "def descargar_youtube(url, progress=gr.Progress()):\n",
        "    \"\"\"Descarga video de YouTube y retorna la ruta temporal\"\"\"\n",
        "    try:\n",
        "        progress(0.1, \"Descargando video de YouTube...\")\n",
        "\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "        output_path = os.path.join(temp_dir, 'video.mp4')\n",
        "\n",
        "        ydl_opts = {\n",
        "            'format': 'best[ext=mp4]',\n",
        "            'outtmpl': output_path,\n",
        "            'quiet': True,\n",
        "            'no_warnings': True,\n",
        "        }\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([url])\n",
        "\n",
        "        progress(0.3, \"Video descargado exitosamente\")\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"Error al descargar video: {str(e)}\")\n",
        "\n",
        "def identificar_victima_y_agresor(\n",
        "    video_input=None,\n",
        "    youtube_url=\"\",\n",
        "    aggressor_threshold=2.8,\n",
        "    victim_threshold=1.5,\n",
        "    proximity_threshold=140,\n",
        "    min_duration_frames=15,\n",
        "    dist_threshold_px=85,\n",
        "    speed_threshold=0.04,\n",
        "    fist_distance_threshold=0.08,\n",
        "    face_cover_threshold=0.10,\n",
        "    crouch_threshold=0.25,\n",
        "    movement_history=15,\n",
        "    mostrar_interaccion=\"Ambos\",\n",
        "    progress=gr.Progress()\n",
        "):\n",
        "    \"\"\"Funci√≥n principal que procesa el video y detecta agresor/v√≠ctima - VERSI√ìN CORREGIDA\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Actualizar par√°metros globales\n",
        "        update_global_params({\n",
        "            \"AGGRESSOR_THRESHOLD\": aggressor_threshold,\n",
        "            \"VICTIM_THRESHOLD\": victim_threshold,\n",
        "            \"PROXIMITY_THRESHOLD\": proximity_threshold,\n",
        "            \"MIN_DURATION_FRAMES\": min_duration_frames,\n",
        "            \"dist_threshold_px\": dist_threshold_px,\n",
        "            \"speed_threshold\": speed_threshold,\n",
        "            \"FIST_DISTANCE_THRESHOLD\": fist_distance_threshold,\n",
        "            \"FACE_COVER_THRESHOLD\": face_cover_threshold,\n",
        "            \"CROUCH_THRESHOLD\": crouch_threshold,\n",
        "            \"movement_history\": movement_history,\n",
        "        })\n",
        "\n",
        "        video_path = None\n",
        "        temp_file = None\n",
        "\n",
        "        # Determinar fuente de video\n",
        "        if youtube_url and youtube_url.strip():\n",
        "            progress(0.1, \"Descargando video de YouTube...\")\n",
        "            video_path = descargar_youtube(youtube_url, progress)\n",
        "            temp_file = video_path\n",
        "        elif video_input is not None:\n",
        "            video_path = video_input\n",
        "        else:\n",
        "            raise gr.Error(\"Por favor, sube un video o ingresa una URL de YouTube\")\n",
        "\n",
        "        progress(0.3, \"Iniciando an√°lisis del video...\")\n",
        "\n",
        "        # Procesar video\n",
        "        (\n",
        "            num_agresores,\n",
        "            num_victimas,\n",
        "            frames_procesados,\n",
        "            video_procesado_path,\n",
        "            frames_con_agresor,\n",
        "            frames_con_victima,\n",
        "            eventos,\n",
        "            img_agresor,\n",
        "            img_victima,\n",
        "            img_ambos\n",
        "        ) = process_video_full_analysis(\n",
        "            video_path,\n",
        "            output_dir='outputs_rwf',\n",
        "            max_frames=50,  # Limitar para prueba r√°pida\n",
        "            use_advanced=True,\n",
        "            yolo_model=yolo_model,\n",
        "            progress=progress\n",
        "        )\n",
        "\n",
        "        progress(0.9, \"Generando resultados...\")\n",
        "\n",
        "        # Seleccionar imagen para mostrar\n",
        "        img_mostrar = None\n",
        "        if mostrar_interaccion == \"Agresor\" and img_agresor and os.path.exists(img_agresor):\n",
        "            img_mostrar = img_agresor\n",
        "        elif mostrar_interaccion == \"V√≠ctima\" and img_victima and os.path.exists(img_victima):\n",
        "            img_mostrar = img_victima\n",
        "        elif mostrar_interaccion == \"Ambos\" and img_ambos and os.path.exists(img_ambos):\n",
        "            img_mostrar = img_ambos\n",
        "\n",
        "        # Generar reporte\n",
        "        reporte = f\"\"\"\n",
        "## üìä Resultados del An√°lisis\n",
        "\n",
        "### Detecci√≥n de Personas\n",
        "- **Agresores √∫nicos detectados:** {num_agresores}\n",
        "- **V√≠ctimas √∫nicas detectadas:** {num_victimas}\n",
        "- **Frames procesados:** {frames_procesados}\n",
        "\n",
        "### Actividad Detectada\n",
        "- **Frames con agresor:** {frames_con_agresor}\n",
        "- **Frames con v√≠ctima:** {frames_con_victima}\n",
        "\n",
        "### Configuraci√≥n\n",
        "- **Umbral agresor:** {aggressor_threshold}\n",
        "- **Umbral v√≠ctima:** {victim_threshold}\n",
        "- **Duraci√≥n m√≠nima:** {min_duration_frames} frames\n",
        "\"\"\"\n",
        "\n",
        "        # Clasificaci√≥n ML (si est√° disponible)\n",
        "        if violence_classifier is not None:\n",
        "            try:\n",
        "                features = extract_lightweight_features(video_path, yolo_model=yolo_model)\n",
        "                prediccion = violence_classifier.predict([features])[0]\n",
        "                probabilidad = violence_classifier.predict_proba([features])[0]\n",
        "\n",
        "                if prediccion == 1:\n",
        "                    reporte += f\"\\n### üî¥ VIOLENCIA DETECTADA\\n- **Confianza:** {probabilidad[1]:.1%}\"\n",
        "                else:\n",
        "                    reporte += f\"\\n### ‚úÖ NO VIOLENCIA\\n- **Confianza:** {probabilidad[0]:.1%}\"\n",
        "            except Exception as e:\n",
        "                reporte += f\"\\n### ‚ö† Clasificaci√≥n ML no disponible\\n- Error: {str(e)}\"\n",
        "        else:\n",
        "            reporte += \"\\n### ‚ö† Clasificador ML no cargado\"\n",
        "\n",
        "        # Limpiar archivos temporales\n",
        "        if temp_file and os.path.exists(temp_file):\n",
        "            try:\n",
        "                os.remove(temp_file)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        progress(1.0, \"¬°An√°lisis completado!\")\n",
        "\n",
        "        return video_procesado_path, img_mostrar, reporte\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"Error durante el procesamiento: {str(e)}\"\n",
        "        print(f\"ERROR: {error_msg}\")\n",
        "        print(traceback.format_exc())\n",
        "        raise gr.Error(error_msg)\n",
        "\n",
        "# ==========================================================================================================\n",
        "# CARGAR MODELOS\n",
        "# ==========================================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ CARGANDO MODELOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Cargando modelo YOLO...\")\n",
        "try:\n",
        "    yolo_model = YOLO('yolov8n-pose.pt')\n",
        "    print(\"‚úì Modelo YOLO cargado\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error cargando YOLO: {e}\")\n",
        "    yolo_model = None\n",
        "\n",
        "# Intentar cargar clasificador de violencia\n",
        "model_path = 'violence_classifier.pkl'\n",
        "violence_classifier = None\n",
        "if os.path.exists(model_path):\n",
        "    try:\n",
        "        violence_classifier = joblib.load(model_path)\n",
        "        print(\"‚úì Clasificador de violencia cargado\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Error cargando clasificador: {e}\")\n",
        "else:\n",
        "    print(\"‚ö† No se encontr√≥ violence_classifier.pkl - El sistema funcionar√° sin clasificaci√≥n ML\")\n",
        "\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ==========================================================================================================\n",
        "# INTERFAZ GRADIO\n",
        "# ==========================================================================================================\n",
        "\n",
        "def create_interface():\n",
        "    \"\"\"Crea y retorna la interfaz de Gradio\"\"\"\n",
        "\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"Identificador V√≠ctima y Agresor\") as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # üéØ Identificador V√≠ctima y Agresor\n",
        "        ### Sistema de Detecci√≥n de Violencia con IA\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### üìÅ Fuente de Video\")\n",
        "\n",
        "                with gr.Tab(\"Subir Video\"):\n",
        "                    video_input = gr.Video(label=\"Seleccionar video\", sources=[\"upload\"])\n",
        "\n",
        "                with gr.Tab(\"YouTube URL\"):\n",
        "                    youtube_url = gr.Textbox(\n",
        "                        label=\"URL de YouTube\",\n",
        "                        placeholder=\"https://www.youtube.com/watch?v=...\",\n",
        "                        lines=1\n",
        "                    )\n",
        "\n",
        "                gr.Markdown(\"### ‚öôÔ∏è Par√°metros de Detecci√≥n\")\n",
        "\n",
        "                with gr.Accordion(\"Par√°metros Principales\", open=True):\n",
        "                    aggressor_threshold = gr.Slider(\n",
        "                        minimum=0.5, maximum=5.0, value=2.8, step=0.1,\n",
        "                        label=\"Umbral Agresor\"\n",
        "                    )\n",
        "                    victim_threshold = gr.Slider(\n",
        "                        minimum=0.5, maximum=5.0, value=1.5, step=0.1,\n",
        "                        label=\"Umbral V√≠ctima\"\n",
        "                    )\n",
        "                    proximity_threshold = gr.Slider(\n",
        "                        minimum=50, maximum=300, value=140, step=10,\n",
        "                        label=\"Umbral de Proximidad (px)\"\n",
        "                    )\n",
        "                    min_duration_frames = gr.Slider(\n",
        "                        minimum=5, maximum=100, value=15, step=5,\n",
        "                        label=\"Duraci√≥n M√≠nima (frames)\"\n",
        "                    )\n",
        "\n",
        "                with gr.Accordion(\"Par√°metros Avanzados\", open=False):\n",
        "                    dist_threshold_px = gr.Slider(\n",
        "                        minimum=20, maximum=200, value=85, step=5,\n",
        "                        label=\"Distancia para Golpes (px)\"\n",
        "                    )\n",
        "                    speed_threshold = gr.Slider(\n",
        "                        minimum=0.01, maximum=0.2, value=0.04, step=0.01,\n",
        "                        label=\"Umbral de Velocidad\"\n",
        "                    )\n",
        "                    fist_distance_threshold = gr.Slider(\n",
        "                        minimum=0.05, maximum=0.3, value=0.08, step=0.01,\n",
        "                        label=\"Detecci√≥n de Pu√±os\"\n",
        "                    )\n",
        "                    face_cover_threshold = gr.Slider(\n",
        "                        minimum=0.05, maximum=0.3, value=0.10, step=0.01,\n",
        "                        label=\"Protecci√≥n Facial\"\n",
        "                    )\n",
        "                    crouch_threshold = gr.Slider(\n",
        "                        minimum=0.1, maximum=0.5, value=0.25, step=0.01,\n",
        "                        label=\"Postura Agachada\"\n",
        "                    )\n",
        "                    movement_history = gr.Slider(\n",
        "                        minimum=5, maximum=50, value=15, step=5,\n",
        "                        label=\"Historial de Movimiento\"\n",
        "                    )\n",
        "\n",
        "                mostrar_interaccion = gr.Radio(\n",
        "                    choices=[\"Ambos\", \"Agresor\", \"V√≠ctima\"],\n",
        "                    value=\"Ambos\",\n",
        "                    label=\"üì∏ Mostrar en Resultados\"\n",
        "                )\n",
        "\n",
        "                btn_analizar = gr.Button(\n",
        "                    \"üöÄ Iniciar An√°lisis\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### üìä Resultados\")\n",
        "\n",
        "                with gr.Tab(\"Video Procesado\"):\n",
        "                    output_video = gr.Video(label=\"Video Analizado\")\n",
        "\n",
        "                with gr.Tab(\"Captura\"):\n",
        "                    output_screenshot = gr.Image(label=\"Primera Interacci√≥n Detectada\")\n",
        "\n",
        "                with gr.Tab(\"Reporte\"):\n",
        "                    output_text = gr.Markdown()\n",
        "\n",
        "        # Conectar eventos\n",
        "        btn_analizar.click(\n",
        "            fn=identificar_victima_y_agresor,\n",
        "            inputs=[\n",
        "                video_input, youtube_url,\n",
        "                aggressor_threshold, victim_threshold,\n",
        "                proximity_threshold, min_duration_frames,\n",
        "                dist_threshold_px, speed_threshold,\n",
        "                fist_distance_threshold, face_cover_threshold,\n",
        "                crouch_threshold, movement_history,\n",
        "                mostrar_interaccion\n",
        "            ],\n",
        "            outputs=[output_video, output_screenshot, output_text]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### ‚ÑπÔ∏è Instrucciones:\n",
        "        1. Sube un video o ingresa una URL de YouTube\n",
        "        2. Ajusta los par√°metros seg√∫n necesites\n",
        "        3. Haz clic en 'Iniciar An√°lisis'\n",
        "        4. Revisa los resultados en las pesta√±as\n",
        "\n",
        "        **Nota:** El procesamiento puede tomar varios minutos dependiendo de la duraci√≥n del video.\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# ==========================================================================================================\n",
        "# LANZAR APLICACI√ìN\n",
        "# ==========================================================================================================\n",
        "# ==========================================================================================================\n",
        "# LANZAR EN COLAB - VERSI√ìN OPTIMIZADA\n",
        "# ==========================================================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Iniciando aplicaci√≥n...\")\n",
        "\n",
        "    # Crear interfaz\n",
        "    demo = create_interface()\n",
        "\n",
        "    # Detectar si estamos en Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"üèÅ Entorno: Google Colab detectado\")\n",
        "        # En Colab, usar share=True para obtener URL p√∫blica\n",
        "        demo.launch(share=True, debug=False)\n",
        "    else:\n",
        "        print(\"üèÅ Entorno: Local/Jupyter detectado\")\n",
        "        # En local, buscar puerto disponible\n",
        "        import socket\n",
        "        from contextlib import closing\n",
        "\n",
        "        def find_free_port():\n",
        "            with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "                s.bind(('', 0))\n",
        "                s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "                return s.getsockname()[1]\n",
        "\n",
        "        free_port = find_free_port()\n",
        "        print(f\"‚úÖ Usando puerto: {free_port}\")\n",
        "\n",
        "        demo.launch(\n",
        "            server_name=\"0.0.0.0\",\n",
        "            server_port=free_port,\n",
        "            share=False,\n",
        "            inbrowser=True,\n",
        "            show_error=True\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "HgaBu4R2j7zT",
        "outputId": "a66e3b94-088e-45dd-8141-edda1c03b8dc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üöÄ CARGANDO MODELOS\n",
            "================================================================================\n",
            "Cargando modelo YOLO...\n",
            "‚úì Modelo YOLO cargado\n",
            "‚úì Clasificador de violencia cargado\n",
            "================================================================================\n",
            "\n",
            "Iniciando aplicaci√≥n...\n",
            "üèÅ Entorno: Google Colab detectado\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d72d99bd870cc64520.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d72d99bd870cc64520.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================================================\n",
        "# PAR√ÅMETROS GLOBALES CONFIGURABLES (ajustados seg√∫n recomendaciones)\n",
        "# ==========================================================================================================\n",
        "GLOBAL_PARAMS = {\n",
        "    \"AGGRESSOR_THRESHOLD\": 2.8,\n",
        "    \"VICTIM_THRESHOLD\": 1.5,\n",
        "    \"PROXIMITY_THRESHOLD\": 140,\n",
        "    \"MIN_DURATION_FRAMES\": 15,\n",
        "    \"dist_threshold_px\": 85,\n",
        "    \"speed_threshold\": 0.07,\n",
        "    \"FIST_DISTANCE_THRESHOLD\": 0.12,\n",
        "    \"FACE_COVER_THRESHOLD\": 0.10,\n",
        "    \"CROUCH_THRESHOLD\": 0.18,\n",
        "    \"disappearance_threshold\": 25,\n",
        "    \"movement_history\": 15,\n",
        "    \"SMOOTH_WINDOW\": 12,\n",
        "}\n",
        "\n",
        "def update_global_params(new_params):\n",
        "    \"\"\"Actualiza los par√°metros globales\"\"\"\n",
        "    GLOBAL_PARAMS.update(new_params)\n",
        "\n",
        "# ==========================================================================================================\n",
        "# UTILIDADES MEJORADAS\n",
        "# ==========================================================================================================\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    try:\n",
        "        a, b, c = np.array(a, dtype=float), np.array(b, dtype=float), np.array(c, dtype=float)\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        denom = (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n",
        "        cosine_angle = np.dot(ba, bc) / denom\n",
        "        cosine_angle = np.clip(cosine_angle, -1, 1)\n",
        "        return np.degrees(np.arccos(cosine_angle))\n",
        "    except Exception:\n",
        "        return 180.0\n",
        "\n",
        "def safe_kp(kp_list, idx):\n",
        "    \"\"\"Devuelve (x,y,conf) o [0,0,0]\"\"\"\n",
        "    try:\n",
        "        return kp_list[idx]\n",
        "    except Exception:\n",
        "        return np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "def detect_punch_direction(wrist, prev_wrist, target_torso, frame_scale=1.0):\n",
        "    \"\"\"Retorna intensidad estimada de golpe dirigido hacia target_torso\"\"\"\n",
        "    try:\n",
        "        if wrist[2] < 0.3 or prev_wrist[2] < 0.3:\n",
        "            return 0.0\n",
        "        v = np.array(wrist[:2]) - np.array(prev_wrist[:2])\n",
        "        speed = np.linalg.norm(v)\n",
        "        to_target = np.array(target_torso) - np.array(wrist[:2])\n",
        "        if np.linalg.norm(to_target) == 0:\n",
        "            return 0.0\n",
        "        angle = calculate_angle([0, 0], v, to_target)\n",
        "        if speed > 6 and angle < 35:\n",
        "            return (speed / (frame_scale + 1e-6)) * 0.12\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def detect_hook_punch(elbow, prev_elbow):\n",
        "    \"\"\"Detecta movimiento circular en codo (gancho)\"\"\"\n",
        "    try:\n",
        "        if elbow[2] < 0.3 or prev_elbow[2] < 0.3:\n",
        "            return 0.0\n",
        "        v = np.array(elbow[:2]) - np.array(prev_elbow[:2])\n",
        "        angular_speed = np.linalg.norm(v)\n",
        "        return 1.2 if angular_speed > 4 else 0.0\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "def detect_push_posture(keypoints, prev_keypoints, img_height):\n",
        "    NOSE, L_HIP, R_HIP = 0, 11, 12\n",
        "    try:\n",
        "        if keypoints[NOSE][2] < 0.3 or keypoints[L_HIP][2] < 0.3 or keypoints[R_HIP][2] < 0.3:\n",
        "            return 0.0\n",
        "        torso_now = (keypoints[L_HIP][:2] + keypoints[R_HIP][:2]) / 2\n",
        "        nose = np.array(keypoints[NOSE][:2])\n",
        "        lean_dist = np.linalg.norm(nose - torso_now)\n",
        "        if prev_keypoints is None or prev_keypoints[NOSE][2] < 0.3:\n",
        "            return 0.0\n",
        "        prev_nose = prev_keypoints[NOSE][:2]\n",
        "        lean_speed = np.linalg.norm(nose - prev_nose)\n",
        "        if lean_dist > 0.06 * img_height and lean_speed > 2:\n",
        "            return lean_speed * 0.3\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def compute_acceleration(history_deque):\n",
        "    \"\"\"Calcula aceleraci√≥n desde historial de posiciones\"\"\"\n",
        "    try:\n",
        "        if len(history_deque) < 3:\n",
        "            return 0.0\n",
        "        p1 = np.array(history_deque[-3])\n",
        "        p2 = np.array(history_deque[-2])\n",
        "        p3 = np.array(history_deque[-1])\n",
        "        v1 = np.linalg.norm(p2 - p1)\n",
        "        v2 = np.linalg.norm(p3 - p2)\n",
        "        return v2 - v1\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "def hand_to_body_distance(hand, target_box):\n",
        "    try:\n",
        "        hx, hy = hand[:2]\n",
        "        tx1, ty1, w, h = target_box\n",
        "        cx, cy = tx1 + w / 2.0, ty1 + h / 2.0\n",
        "        return np.linalg.norm([hx - cx, hy - cy])\n",
        "    except Exception:\n",
        "        return float('inf')\n",
        "\n",
        "def detect_collapse(keypoints, prev_keypoints):\n",
        "    \"\"\"Detecta ca√≠da brusca (knockdown)\"\"\"\n",
        "    try:\n",
        "        L_HIP, R_HIP = 11, 12\n",
        "        if keypoints[L_HIP][2] < 0.3 or prev_keypoints is None or prev_keypoints[L_HIP][2] < 0.3:\n",
        "            return 0.0\n",
        "        hip_now = (keypoints[L_HIP][:2] + keypoints[R_HIP][:2]) / 2\n",
        "        hip_prev = (prev_keypoints[L_HIP][:2] + prev_keypoints[R_HIP][:2]) / 2\n",
        "        dy = hip_prev[1] - hip_now[1]\n",
        "        if dy < -20:\n",
        "            return abs(dy) * 0.05\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "# ==========================================================================================================\n",
        "# DETECTORES DE POSTURAS MEJORADOS\n",
        "# ==========================================================================================================\n",
        "\n",
        "def detect_offensive_postures(keypoints, box, img_height, prev_keypoints=None, target_boxes=None):\n",
        "    \"\"\"Detecci√≥n mejorada de posturas ofensivas\"\"\"\n",
        "    features = {'arms_raised': 0.0, 'fists_closed': 0.0, 'forward_lean': 0.0,\n",
        "                'offensive_gesture': 0.0, 'punch_direction': 0.0, 'hook': 0.0, 'push': 0.0,\n",
        "                'total_score': 0.0}\n",
        "\n",
        "    CONFIDENCE_THRESHOLD = 0.3\n",
        "    FIST_DISTANCE_THRESHOLD = GLOBAL_PARAMS[\"FIST_DISTANCE_THRESHOLD\"]\n",
        "    NOSE, L_SHOULDER, R_SHOULDER = 0, 5, 6\n",
        "    L_ELBOW, R_ELBOW, L_WRIST, R_WRIST = 7, 8, 9, 10\n",
        "\n",
        "    try:\n",
        "        # Brazos levantados\n",
        "        if keypoints[L_WRIST][2] > CONFIDENCE_THRESHOLD and keypoints[L_SHOULDER][2] > CONFIDENCE_THRESHOLD:\n",
        "            if keypoints[L_WRIST][1] < keypoints[L_SHOULDER][1]:\n",
        "                features['arms_raised'] += 1.0\n",
        "        if keypoints[R_WRIST][2] > CONFIDENCE_THRESHOLD and keypoints[R_SHOULDER][2] > CONFIDENCE_THRESHOLD:\n",
        "            if keypoints[R_WRIST][1] < keypoints[R_SHOULDER][1]:\n",
        "                features['arms_raised'] += 1.0\n",
        "\n",
        "        # Pu√±os cerrados\n",
        "        if keypoints[L_WRIST][2] > CONFIDENCE_THRESHOLD and keypoints[L_ELBOW][2] > CONFIDENCE_THRESHOLD:\n",
        "            dist_le = np.linalg.norm(keypoints[L_WRIST][:2] - keypoints[L_ELBOW][:2])\n",
        "            if dist_le < FIST_DISTANCE_THRESHOLD * img_height:\n",
        "                features['fists_closed'] += 1.5\n",
        "        if keypoints[R_WRIST][2] > CONFIDENCE_THRESHOLD and keypoints[R_ELBOW][2] > CONFIDENCE_THRESHOLD:\n",
        "            dist_re = np.linalg.norm(keypoints[R_WRIST][:2] - keypoints[R_ELBOW][:2])\n",
        "            if dist_re < FIST_DISTANCE_THRESHOLD * img_height:\n",
        "                features['fists_closed'] += 1.5\n",
        "\n",
        "        # Inclinaci√≥n hacia adelante\n",
        "        if all(keypoints[i][2] > CONFIDENCE_THRESHOLD for i in [NOSE, L_SHOULDER, R_SHOULDER]):\n",
        "            shoulder_center = (keypoints[L_SHOULDER][:2] + keypoints[R_SHOULDER][:2]) / 2.0\n",
        "            if keypoints[NOSE][0] > shoulder_center[0]:\n",
        "                features['forward_lean'] += 1.0\n",
        "\n",
        "        # Gestos ofensivos por √°ngulos\n",
        "        for shoulder, elbow, wrist in [(L_SHOULDER, L_ELBOW, L_WRIST), (R_SHOULDER, R_ELBOW, R_WRIST)]:\n",
        "            if all(keypoints[i][2] > CONFIDENCE_THRESHOLD for i in [shoulder, elbow, wrist]):\n",
        "                angle = calculate_angle(keypoints[shoulder][:2], keypoints[elbow][:2], keypoints[wrist][:2])\n",
        "                if 150 < angle < 180:\n",
        "                    features['offensive_gesture'] += 2.0\n",
        "\n",
        "        # Detecci√≥n de golpes dirigidos\n",
        "        if prev_keypoints is not None and target_boxes is not None:\n",
        "            for wrist_idx in [L_WRIST, R_WRIST]:\n",
        "                prev_w = prev_keypoints[wrist_idx]\n",
        "                curr_w = keypoints[wrist_idx]\n",
        "                if curr_w[2] > CONFIDENCE_THRESHOLD and prev_w[2] > CONFIDENCE_THRESHOLD:\n",
        "                    for tb in target_boxes:\n",
        "                        center_tb = (tb[0] + tb[2] / 2.0, tb[1] + tb[3] / 2.0)\n",
        "                        punch_score = detect_punch_direction(curr_w, prev_w, center_tb, frame_scale=img_height)\n",
        "                        if punch_score > 0:\n",
        "                            features['punch_direction'] += punch_score\n",
        "\n",
        "                    # Detecci√≥n de gancho\n",
        "                    elbow_idx = L_ELBOW if wrist_idx == L_WRIST else R_ELBOW\n",
        "                    prev_el = prev_keypoints[elbow_idx]\n",
        "                    curr_el = keypoints[elbow_idx]\n",
        "                    features['hook'] += detect_hook_punch(curr_el, prev_el)\n",
        "\n",
        "        # Postura de empuje\n",
        "        if prev_keypoints is not None:\n",
        "            features['push'] += detect_push_posture(keypoints, prev_keypoints, img_height)\n",
        "\n",
        "        features['total_score'] = (features['arms_raised'] + features['fists_closed'] +\n",
        "                                   features['forward_lean'] + features['offensive_gesture'] +\n",
        "                                   features['punch_direction'] + features['hook'] + features['push'])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return features\n",
        "\n",
        "def detect_defensive_postures(keypoints, box, img_height, prev_keypoints=None):\n",
        "    \"\"\"Detecci√≥n mejorada de posturas defensivas\"\"\"\n",
        "    features = {'covering_face': 0.0, 'crouching': 0.0, 'defensive_arms': 0.0, 'collapse': 0.0, 'total_score': 0.0}\n",
        "\n",
        "    CONFIDENCE_THRESHOLD = 0.3\n",
        "    FACE_COVER_THRESHOLD = GLOBAL_PARAMS[\"FACE_COVER_THRESHOLD\"]\n",
        "    CROUCH_THRESHOLD = GLOBAL_PARAMS[\"CROUCH_THRESHOLD\"]\n",
        "    NOSE, L_EYE, R_EYE, L_EAR, R_EAR = 0, 1, 2, 3, 4\n",
        "    L_SHOULDER, R_SHOULDER, L_ELBOW, R_ELBOW = 5, 6, 7, 8\n",
        "    L_WRIST, R_WRIST, L_HIP, R_HIP = 9, 10, 11, 12\n",
        "\n",
        "    try:\n",
        "        # Protecci√≥n facial\n",
        "        face_points = [NOSE, L_EYE, R_EYE, L_EAR, R_EAR]\n",
        "        valid_face_points = [keypoints[i][:2] for i in face_points if keypoints[i][2] > CONFIDENCE_THRESHOLD]\n",
        "        if valid_face_points:\n",
        "            face_center = np.mean(valid_face_points, axis=0)\n",
        "            for wrist in [L_WRIST, R_WRIST]:\n",
        "                if keypoints[wrist][2] > CONFIDENCE_THRESHOLD:\n",
        "                    dist = np.linalg.norm(keypoints[wrist][:2] - face_center)\n",
        "                    if dist < FACE_COVER_THRESHOLD * img_height:\n",
        "                        features['covering_face'] += 2.0\n",
        "\n",
        "        # Postura agachada\n",
        "        if keypoints[L_SHOULDER][2] > CONFIDENCE_THRESHOLD and keypoints[L_HIP][2] > CONFIDENCE_THRESHOLD:\n",
        "            upper_body_height = abs(keypoints[L_SHOULDER][1] - keypoints[L_HIP][1])\n",
        "            if upper_body_height < CROUCH_THRESHOLD * img_height:\n",
        "                features['crouching'] += 1.5\n",
        "\n",
        "        # Brazos defensivos\n",
        "        for shoulder, elbow, wrist in [(L_SHOULDER, L_ELBOW, L_WRIST), (R_SHOULDER, R_ELBOW, R_WRIST)]:\n",
        "            if all(keypoints[i][2] > CONFIDENCE_THRESHOLD for i in [shoulder, elbow, wrist]):\n",
        "                angle = calculate_angle(keypoints[shoulder][:2], keypoints[elbow][:2], keypoints[wrist][:2])\n",
        "                if 60 < angle < 120:\n",
        "                    features['defensive_arms'] += 1.0\n",
        "\n",
        "        # Colapso/ca√≠da\n",
        "        if prev_keypoints is not None:\n",
        "            features['collapse'] += detect_collapse(keypoints, prev_keypoints)\n",
        "\n",
        "        features['total_score'] = features['covering_face'] + features['crouching'] + features['defensive_arms'] + features['collapse']\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return features\n",
        "\n",
        "# ==========================================================================================================\n",
        "# SISTEMA DE AN√ÅLISIS MEJORADO - VERSI√ìN SIMPLIFICADA PERO FUNCIONAL\n",
        "# ==========================================================================================================\n",
        "\n",
        "def analyze_interactions_simple(person_boxes, keypoints_list, frame_height):\n",
        "    \"\"\"Versi√≥n simplificada pero funcional del an√°lisis de interacciones\"\"\"\n",
        "    interactions = {'proximity_scores': {}, 'aggression_vectors': {}}\n",
        "\n",
        "    try:\n",
        "        for i, (box_i, kp_i) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            interactions['proximity_scores'][i] = 0.0\n",
        "            interactions['aggression_vectors'][i] = 0.0\n",
        "\n",
        "            center_i = np.array([box_i[0] + box_i[2]/2.0, box_i[1] + box_i[3]/2.0])\n",
        "            offensive_score = detect_offensive_postures(kp_i, box_i, frame_height).get('total_score', 0.0)\n",
        "\n",
        "            for j, (box_j, kp_j) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                center_j = np.array([box_j[0] + box_j[2]/2.0, box_j[1] + box_j[3]/2.0])\n",
        "                distance = np.linalg.norm(center_i - center_j)\n",
        "\n",
        "                # Proximidad\n",
        "                if distance < GLOBAL_PARAMS[\"PROXIMITY_THRESHOLD\"]:\n",
        "                    proximity_score = (GLOBAL_PARAMS[\"PROXIMITY_THRESHOLD\"] - distance) / GLOBAL_PARAMS[\"PROXIMITY_THRESHOLD\"]\n",
        "                    interactions['proximity_scores'][i] += proximity_score\n",
        "\n",
        "                # Agresi√≥n basada en postura ofensiva y proximidad\n",
        "                if offensive_score > 1.0 and distance < 200:\n",
        "                    interactions['aggression_vectors'][i] += offensive_score * 0.5\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en analyze_interactions_simple: {e}\")\n",
        "\n",
        "    return interactions\n",
        "\n",
        "def assign_roles_simple(person_boxes, keypoints_list, frame_height):\n",
        "    \"\"\"Asignaci√≥n simplificada de roles que S√ç funciona\"\"\"\n",
        "    roles = []\n",
        "\n",
        "    try:\n",
        "        if len(person_boxes) < 2:\n",
        "            return roles\n",
        "\n",
        "        interactions = analyze_interactions_simple(person_boxes, keypoints_list, frame_height)\n",
        "\n",
        "        for i, (box, kp) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            offensive_score = detect_offensive_postures(kp, box, frame_height).get('total_score', 0.0)\n",
        "            defensive_score = detect_defensive_postures(kp, box, frame_height).get('total_score', 0.0)\n",
        "            proximity_score = interactions['proximity_scores'].get(i, 0.0)\n",
        "            aggression_score = interactions['aggression_vectors'].get(i, 0.0)\n",
        "\n",
        "            # Puntuaci√≥n final m√°s simple pero efectiva\n",
        "            final_score = (offensive_score * 1.5 + aggression_score * 1.2 +\n",
        "                          proximity_score * 0.8 - defensive_score * 0.5)\n",
        "\n",
        "            victim_score = (defensive_score * 1.5 + proximity_score * 0.6 -\n",
        "                           offensive_score * 0.3)\n",
        "\n",
        "            if final_score >= GLOBAL_PARAMS[\"AGGRESSOR_THRESHOLD\"]:\n",
        "                roles.append((i, 'agresor', final_score))\n",
        "            elif victim_score >= GLOBAL_PARAMS[\"VICTIM_THRESHOLD\"]:\n",
        "                roles.append((i, 'victima', victim_score))\n",
        "            else:\n",
        "                roles.append((i, 'desconocido', final_score))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en assign_roles_simple: {e}\")\n",
        "\n",
        "    return roles\n",
        "\n",
        "# ==========================================================================================================\n",
        "# PROCESAMIENTO DE VIDEO MEJORADO - VERSI√ìN QUE S√ç FUNCIONA\n",
        "# ==========================================================================================================\n",
        "\n",
        "def process_video_full_analysis(video_input, output_dir='outputs_rwf', max_frames=None, use_advanced=True, yolo_model=None, progress=None):\n",
        "    \"\"\"Funci√≥n principal MEJORADA que S√ç detecta agresores y v√≠ctimas\"\"\"\n",
        "    try:\n",
        "        if progress is not None:\n",
        "            progress(0, \"üîÑ Preparando video...\")\n",
        "\n",
        "        # Manejar entrada de video\n",
        "        if isinstance(video_input, str) and os.path.exists(video_input):\n",
        "            video_path = video_input\n",
        "            video_name = os.path.basename(video_input).split('.')[0]\n",
        "        else:\n",
        "            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:\n",
        "                if hasattr(video_input, 'read'):\n",
        "                    temp_file.write(video_input.read())\n",
        "                video_path = temp_file.name\n",
        "            video_name = \"video_temp\"\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return 0, 0, 0, None, 0, 0, [], None, None, None\n",
        "\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS)) if cap.get(cv2.CAP_PROP_FPS) > 0 else 25\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # ‚úÖ CORRECCI√ìN IMPORTANTE: Procesar M√ÅS frames (no solo 50)\n",
        "        if max_frames is None:\n",
        "            max_frames = total_frames  # Procesar todo el video\n",
        "        else:\n",
        "            max_frames = min(max_frames, total_frames)  # L√≠mite razonable\n",
        "\n",
        "        # Configurar salida\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, f\"{video_name}_processed.avi\")\n",
        "        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (640, 480))\n",
        "\n",
        "        # Variables de seguimiento MEJORADAS\n",
        "        frame_count = 0\n",
        "        processed_frame_idx = 0\n",
        "        unique_aggressors = set()\n",
        "        unique_victims = set()\n",
        "        aggressor_frame_count = 0\n",
        "        victim_frame_count = 0\n",
        "\n",
        "        # Capturas\n",
        "        first_agg_path = first_vic_path = first_both_path = None\n",
        "        screenshot_saved_agg = screenshot_saved_vic = screenshot_saved_both = False\n",
        "\n",
        "        if progress is not None:\n",
        "            progress(0.1, \"üéØ Iniciando an√°lisis de frames...\")\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret or frame_count >= max_frames:\n",
        "                break\n",
        "\n",
        "            if progress is not None and frame_count % 10 == 0:\n",
        "                progress(frame_count / max_frames, f\"üìä Procesando frame {frame_count}/{max_frames}\")\n",
        "\n",
        "            # Redimensionar frame\n",
        "            frame_resized = cv2.resize(frame, (640, 480))\n",
        "\n",
        "            # Detectar personas con YOLO\n",
        "            results = yolo_model(frame_resized, verbose=False)\n",
        "\n",
        "            person_boxes = []\n",
        "            keypoints_list = []\n",
        "            current_aggressors = []\n",
        "            current_victims = []\n",
        "\n",
        "            if results and len(results) > 0:\n",
        "                result = results[0]\n",
        "                if result.boxes is not None and result.keypoints is not None:\n",
        "                    for i, box in enumerate(result.boxes):\n",
        "                        if int(box.cls) == 0:  # Solo personas\n",
        "                            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                            conf = box.conf.item()\n",
        "                            kp = result.keypoints.data[i].cpu().numpy() if i < len(result.keypoints.data) else np.zeros((17, 3))\n",
        "\n",
        "                            person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                            keypoints_list.append(kp)\n",
        "\n",
        "            # ‚úÖ AN√ÅLISIS MEJORADO: Usar el sistema de detecci√≥n MEJORADO\n",
        "            if len(person_boxes) >= 2:  # Solo analizar si hay al menos 2 personas\n",
        "                roles = assign_roles_simple(person_boxes, keypoints_list, 480)\n",
        "\n",
        "                for person_idx, role, score in roles:\n",
        "                    if role == 'agresor':\n",
        "                        current_aggressors.append(person_idx)\n",
        "                        unique_aggressors.add(person_idx)\n",
        "                        color = (0, 0, 255)  # Rojo\n",
        "                        label = f\"Agresor ({score:.1f})\"\n",
        "                    elif role == 'victima':\n",
        "                        current_victims.append(person_idx)\n",
        "                        unique_victims.add(person_idx)\n",
        "                        color = (255, 0, 0)  # Azul\n",
        "                        label = f\"Victima ({score:.1f})\"\n",
        "                    else:\n",
        "                        color = (0, 255, 0)  # Verde\n",
        "                        label = \"Desconocido\"\n",
        "\n",
        "                    # Dibujar bounding box y etiqueta\n",
        "                    if person_idx < len(person_boxes):\n",
        "                        x, y, w, h = [int(v) for v in person_boxes[person_idx]]\n",
        "                        cv2.rectangle(frame_resized, (x, y), (x + w, y + h), color, 2)\n",
        "                        cv2.putText(frame_resized, label, (x, y - 10),\n",
        "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "            # Actualizar contadores\n",
        "            if current_aggressors:\n",
        "                aggressor_frame_count += 1\n",
        "            if current_victims:\n",
        "                victim_frame_count += 1\n",
        "\n",
        "            # Guardar capturas MEJORADO\n",
        "            if not screenshot_saved_agg and current_aggressors:\n",
        "                first_agg_path = os.path.join(output_dir, f\"{video_name}_primer_agresor.jpg\")\n",
        "                cv2.imwrite(first_agg_path, frame_resized)\n",
        "                screenshot_saved_agg = True\n",
        "                print(f\"‚úÖ Captura de agresor guardada\")\n",
        "\n",
        "            if not screenshot_saved_vic and current_victims:\n",
        "                first_vic_path = os.path.join(output_dir, f\"{video_name}_primera_victima.jpg\")\n",
        "                cv2.imwrite(first_vic_path, frame_resized)\n",
        "                screenshot_saved_vic = True\n",
        "                print(f\"‚úÖ Captura de v√≠ctima guardada\")\n",
        "\n",
        "            if not screenshot_saved_both and current_aggressors and current_victims:\n",
        "                first_both_path = os.path.join(output_dir, f\"{video_name}_primer_ambos.jpg\")\n",
        "                cv2.imwrite(first_both_path, frame_resized)\n",
        "                screenshot_saved_both = True\n",
        "                print(f\"‚úÖ Captura de interacci√≥n guardada\")\n",
        "\n",
        "            # Informaci√≥n en pantalla\n",
        "            info_text = f\"Frame: {frame_count} | Personas: {len(person_boxes)} | Agresores: {len(current_aggressors)} | Victimas: {len(current_victims)}\"\n",
        "            cv2.putText(frame_resized, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "            # Escribir frame procesado\n",
        "            out.write(frame_resized)\n",
        "            frame_count += 1\n",
        "            processed_frame_idx += 1\n",
        "\n",
        "        # Liberar recursos\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Limpiar archivo temporal si se cre√≥\n",
        "        if 'temp_file' in locals():\n",
        "            try:\n",
        "                os.unlink(video_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        print(f\"‚úÖ An√°lisis completado: {len(unique_aggressors)} agresores, {len(unique_victims)} v√≠ctimas\")\n",
        "\n",
        "        if progress is not None:\n",
        "            progress(1.0, \"‚úÖ An√°lisis completado!\")\n",
        "\n",
        "        return (\n",
        "            len(unique_aggressors),\n",
        "            len(unique_victims),\n",
        "            processed_frame_idx,\n",
        "            output_path,\n",
        "            aggressor_frame_count,\n",
        "            victim_frame_count,\n",
        "            [],  # eventos\n",
        "            first_agg_path,\n",
        "            first_vic_path,\n",
        "            first_both_path\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en process_video_full_analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 0, 0, 0, None, 0, 0, [], None, None, None\n",
        "\n",
        "# ==========================================================================================================\n",
        "# EXTRACTOR DE FEATURES CORREGIDO\n",
        "# ==========================================================================================================\n",
        "\n",
        "def extract_lightweight_features(video_input, max_samples=10, yolo_model=None, progress=None):\n",
        "    \"\"\"Extrae features b√°sicas para clasificaci√≥n ML - VERSI√ìN CORREGIDA\"\"\"\n",
        "    try:\n",
        "        if isinstance(video_input, str) and os.path.exists(video_input):\n",
        "            video_path = video_input\n",
        "        else:\n",
        "            return np.zeros(6)  # Fallback\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return np.zeros(6)\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        sample_frames = min(max_samples, total_frames)\n",
        "\n",
        "        # Features b√°sicas pero reales\n",
        "        person_counts = []\n",
        "        motion_values = []\n",
        "\n",
        "        for i in range(sample_frames):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_resized = cv2.resize(frame, (416, 416))\n",
        "\n",
        "            # Contar personas\n",
        "            results = yolo_model(frame_resized, verbose=False, classes=[0])\n",
        "            person_count = len(results[0].boxes) if results else 0\n",
        "            person_counts.append(person_count)\n",
        "\n",
        "            # Movimiento simple (diferencia de frames)\n",
        "            if i > 0:\n",
        "                gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
        "                prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "                diff = cv2.absdiff(gray, prev_gray)\n",
        "                motion_values.append(np.mean(diff))\n",
        "\n",
        "            prev_frame = frame_resized.copy()\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Crear features\n",
        "        features = np.array([\n",
        "            np.mean(person_counts) if person_counts else 0,\n",
        "            np.max(person_counts) if person_counts else 0,\n",
        "            np.std(person_counts) if person_counts else 0,\n",
        "            np.mean(motion_values) if motion_values else 0,\n",
        "            np.max(motion_values) if motion_values else 0,\n",
        "            total_frames / 1000.0  # Duraci√≥n aproximada\n",
        "        ])\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en extract_lightweight_features: {e}\")\n",
        "        return np.zeros(6)\n",
        "\n",
        "# ==========================================================================================================\n",
        "# FUNCI√ìN PRINCIPAL MEJORADA\n",
        "# ==========================================================================================================\n",
        "\n",
        "def identificar_victima_y_agresor(\n",
        "    video_input=None,\n",
        "    youtube_url=\"\",\n",
        "    aggressor_threshold=2.8,\n",
        "    victim_threshold=1.5,\n",
        "    proximity_threshold=140,\n",
        "    min_duration_frames=15,\n",
        "    dist_threshold_px=85,\n",
        "    speed_threshold=0.04,\n",
        "    fist_distance_threshold=0.08,\n",
        "    face_cover_threshold=0.10,\n",
        "    crouch_threshold=0.25,\n",
        "    movement_history=15,\n",
        "    mostrar_interaccion=\"Ambos\",\n",
        "    progress=gr.Progress()\n",
        "):\n",
        "    \"\"\"Funci√≥n principal MEJORADA con detecci√≥n REAL\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Actualizar par√°metros globales con tus valores MEJORADOS\n",
        "        update_global_params({\n",
        "            \"AGGRESSOR_THRESHOLD\": aggressor_threshold,\n",
        "            \"VICTIM_THRESHOLD\": victim_threshold,\n",
        "            \"PROXIMITY_THRESHOLD\": proximity_threshold,\n",
        "            \"MIN_DURATION_FRAMES\": min_duration_frames,\n",
        "            \"dist_threshold_px\": dist_threshold_px,\n",
        "            \"speed_threshold\": speed_threshold,\n",
        "            \"FIST_DISTANCE_THRESHOLD\": fist_distance_threshold,\n",
        "            \"FACE_COVER_THRESHOLD\": face_cover_threshold,\n",
        "            \"CROUCH_THRESHOLD\": crouch_threshold,\n",
        "            \"movement_history\": movement_history,\n",
        "        })\n",
        "\n",
        "        video_path = None\n",
        "        temp_file = None\n",
        "\n",
        "        # Determinar fuente de video\n",
        "        if youtube_url and youtube_url.strip():\n",
        "            progress(0.1, \"üåê Descargando video de YouTube...\")\n",
        "            video_path = descargar_youtube(youtube_url, progress)\n",
        "            temp_file = video_path\n",
        "        elif video_input is not None:\n",
        "            video_path = video_input\n",
        "        else:\n",
        "            raise gr.Error(\"‚ùå Por favor, sube un video o ingresa una URL de YouTube\")\n",
        "\n",
        "        progress(0.3, \"üéØ Iniciando an√°lisis avanzado del video...\")\n",
        "\n",
        "        # ‚úÖ CORRECCI√ìN: Procesar M√ÅS frames (no solo 50)\n",
        "        (\n",
        "            num_agresores,\n",
        "            num_victimas,\n",
        "            frames_procesados,\n",
        "            video_procesado_path,\n",
        "            frames_con_agresor,\n",
        "            frames_con_victima,\n",
        "            eventos,\n",
        "            img_agresor,\n",
        "            img_victima,\n",
        "            img_ambos\n",
        "        ) = process_video_full_analysis(\n",
        "            video_path,\n",
        "            output_dir='outputs_rwf',\n",
        "            max_frames=None,  # ‚úÖ Procesar TODO el video\n",
        "            use_advanced=True,\n",
        "            yolo_model=yolo_model,\n",
        "            progress=progress\n",
        "        )\n",
        "\n",
        "        progress(0.9, \"üìä Generando resultados...\")\n",
        "\n",
        "        # Seleccionar imagen para mostrar\n",
        "        img_mostrar = None\n",
        "        mensaje_captura = \"\"\n",
        "\n",
        "        if mostrar_interaccion == \"Agresor\" and img_agresor and os.path.exists(img_agresor):\n",
        "            img_mostrar = img_agresor\n",
        "            mensaje_captura = \"üî¥ Primera detecci√≥n de AGRESOR\"\n",
        "        elif mostrar_interaccion == \"V√≠ctima\" and img_victima and os.path.exists(img_victima):\n",
        "            img_mostrar = img_victima\n",
        "            mensaje_captura = \"üîµ Primera detecci√≥n de V√çCTIMA\"\n",
        "        elif mostrar_interaccion == \"Ambos\" and img_ambos and os.path.exists(img_ambos):\n",
        "            img_mostrar = img_ambos\n",
        "            mensaje_captura = \"üë• Primera interacci√≥n AGRESOR + V√çCTIMA\"\n",
        "        else:\n",
        "            mensaje_captura = \"‚ö† No se capturaron interacciones del tipo seleccionado\"\n",
        "\n",
        "        # ‚úÖ REPORTE MEJORADO con informaci√≥n REAL\n",
        "        violencia_detectada = (frames_con_agresor >= min_duration_frames and\n",
        "                              frames_con_victima >= min_duration_frames)\n",
        "\n",
        "        reporte = f\"\"\"\n",
        "## üìä Resultados del An√°lisis\n",
        "\n",
        "### üë• Detecci√≥n de Personas\n",
        "- **Agresores detectados:** {num_agresores}\n",
        "- **V√≠ctimas detectadas:** {num_victimas}\n",
        "- **Frames procesados:** {frames_procesados}\n",
        "\n",
        "### üéØ Actividad Detectada\n",
        "- **Frames con agresor:** {frames_con_agresor}\n",
        "- **Frames con v√≠ctima:** {frames_con_victima}\n",
        "- **Violencia prolongada:** {'‚úÖ S√ç' if violencia_detectada else '‚ùå NO'}\n",
        "\n",
        "### üì∏ Capturas\n",
        "- **{mensaje_captura}**\n",
        "\n",
        "### ‚öôÔ∏è Configuraci√≥n Usada\n",
        "- **Umbral agresor:** {aggressor_threshold}\n",
        "- **Umbral v√≠ctima:** {victim_threshold}\n",
        "- **Proximidad:** {proximity_threshold}px\n",
        "- **Duraci√≥n m√≠nima:** {min_duration_frames} frames\n",
        "\"\"\"\n",
        "\n",
        "        # Clasificaci√≥n ML MEJORADA (manejo de errores)\n",
        "        if violence_classifier is not None:\n",
        "            try:\n",
        "                features = extract_lightweight_features(video_path, yolo_model=yolo_model)\n",
        "                # ‚úÖ CORRECCI√ìN: Manejar el error de √≠ndice\n",
        "                prediccion = violence_classifier.predict([features])[0]\n",
        "                probabilidad = violence_classifier.predict_proba([features])[0]\n",
        "\n",
        "                # Verificar la forma de las probabilidades\n",
        "                if len(probabilidad) >= 2:\n",
        "                    if prediccion == 1:\n",
        "                        reporte += f\"\\n### üî¥ CLASIFICADOR ML: VIOLENCIA DETECTADA\\n- **Confianza:** {probabilidad[1]:.1%}\"\n",
        "                    else:\n",
        "                        reporte += f\"\\n### ‚úÖ CLASIFICADOR ML: NO VIOLENCIA\\n- **Confianza:** {probabilidad[0]:.1%}\"\n",
        "                else:\n",
        "                    reporte += f\"\\n### ‚ö† CLASIFICADOR ML: Resultado - {prediccion}\"\n",
        "\n",
        "            except Exception as e:\n",
        "                reporte += f\"\\n### ‚ö† Clasificaci√≥n ML no disponible\\n- Error: {str(e)}\"\n",
        "        else:\n",
        "            reporte += \"\\n### ‚ÑπÔ∏è Clasificador ML no cargado\"\n",
        "\n",
        "        # Limpiar archivos temporales\n",
        "        if temp_file and os.path.exists(temp_file):\n",
        "            try:\n",
        "                os.remove(temp_file)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        progress(1.0, \"‚úÖ ¬°An√°lisis completado!\")\n",
        "\n",
        "        return video_procesado_path, img_mostrar, reporte\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"Error durante el procesamiento: {str(e)}\"\n",
        "        print(f\"ERROR: {error_msg}\")\n",
        "        print(traceback.format_exc())\n",
        "        raise gr.Error(error_msg)\n",
        "\n",
        "# === LANZAR ===\n",
        "print(\"Iniciando Identificador V√≠ctima y Agresor...\")\n",
        "\n",
        "try:\n",
        "    demo.launch(\n",
        "        share=True,                    # mantiene el link p√∫blico temporal (opcional)\n",
        "        debug=False,                   # <- evita que la celda quede \"colgada\" en debug mode\n",
        "        prevent_thread_lock=True,      # <- crucial para Colab / notebooks\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        inbrowser=False,               # en Colab conviene False; cambia a True si ejecutas localmente\n",
        "        allowed_paths=[\"/content\", \"./\"]\n",
        "    )\n",
        "except KeyboardInterrupt:\n",
        "    # se alcanza si el usuario detiene la celda; cerramos de forma limpia\n",
        "    print(\"Servidor detenido por el usuario (KeyboardInterrupt).\")\n",
        "except Exception as e:\n",
        "    # atrapa otros errores, √∫til para depuraci√≥n en notebooks\n",
        "    print(f\"Error al lanzar Gradio: {e}\")\n",
        "\n",
        "\n",
        "# ==========================================================================================================\n",
        "# CARGAR MODELOS\n",
        "# ==========================================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üöÄ CARGANDO MODELOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Cargando modelo YOLO...\")\n",
        "try:\n",
        "    yolo_model = YOLO('yolov8n-pose.pt')\n",
        "    print(\"‚úì Modelo YOLO cargado\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error cargando YOLO: {e}\")\n",
        "    yolo_model = None\n",
        "\n",
        "# Intentar cargar clasificador de violencia\n",
        "model_path = 'violence_classifier.pkl'\n",
        "violence_classifier = None\n",
        "if os.path.exists(model_path):\n",
        "    try:\n",
        "        violence_classifier = joblib.load(model_path)\n",
        "        print(\"‚úì Clasificador de violencia cargado\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Error cargando clasificador: {e}\")\n",
        "else:\n",
        "    print(\"‚ö† No se encontr√≥ violence_classifier.pkl - El sistema funcionar√° sin clasificaci√≥n ML\")\n",
        "\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ==========================================================================================================\n",
        "# INTERFAZ GRADIO\n",
        "# ==========================================================================================================\n",
        "\n",
        "def create_interface():\n",
        "    \"\"\"Crea y retorna la interfaz de Gradio\"\"\"\n",
        "\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"Identificador V√≠ctima y Agresor\") as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # üéØ Identificador V√≠ctima y Agresor\n",
        "        ### Sistema de Detecci√≥n de Violencia con IA\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### üìÅ Fuente de Video\")\n",
        "\n",
        "                with gr.Tab(\"Subir Video\"):\n",
        "                    video_input = gr.Video(label=\"Seleccionar video\", sources=[\"upload\"])\n",
        "\n",
        "                with gr.Tab(\"YouTube URL\"):\n",
        "                    youtube_url = gr.Textbox(\n",
        "                        label=\"URL de YouTube\",\n",
        "                        placeholder=\"https://www.youtube.com/watch?v=...\",\n",
        "                        lines=1\n",
        "                    )\n",
        "\n",
        "                gr.Markdown(\"### ‚öôÔ∏è Par√°metros de Detecci√≥n\")\n",
        "\n",
        "                with gr.Accordion(\"Par√°metros Principales\", open=True):\n",
        "                    aggressor_threshold = gr.Slider(\n",
        "                        minimum=0.5, maximum=5.0, value=2.8, step=0.1,\n",
        "                        label=\"Umbral Agresor\"\n",
        "                    )\n",
        "                    victim_threshold = gr.Slider(\n",
        "                        minimum=0.5, maximum=5.0, value=1.5, step=0.1,\n",
        "                        label=\"Umbral V√≠ctima\"\n",
        "                    )\n",
        "                    proximity_threshold = gr.Slider(\n",
        "                        minimum=50, maximum=300, value=140, step=10,\n",
        "                        label=\"Umbral de Proximidad (px)\"\n",
        "                    )\n",
        "                    min_duration_frames = gr.Slider(\n",
        "                        minimum=5, maximum=100, value=15, step=5,\n",
        "                        label=\"Duraci√≥n M√≠nima (frames)\"\n",
        "                    )\n",
        "\n",
        "                with gr.Accordion(\"Par√°metros Avanzados\", open=False):\n",
        "                    dist_threshold_px = gr.Slider(\n",
        "                        minimum=20, maximum=200, value=85, step=5,\n",
        "                        label=\"Distancia para Golpes (px)\"\n",
        "                    )\n",
        "                    speed_threshold = gr.Slider(\n",
        "                        minimum=0.01, maximum=0.2, value=0.04, step=0.01,\n",
        "                        label=\"Umbral de Velocidad\"\n",
        "                    )\n",
        "                    fist_distance_threshold = gr.Slider(\n",
        "                        minimum=0.05, maximum=0.3, value=0.08, step=0.01,\n",
        "                        label=\"Detecci√≥n de Pu√±os\"\n",
        "                    )\n",
        "                    face_cover_threshold = gr.Slider(\n",
        "                        minimum=0.05, maximum=0.3, value=0.10, step=0.01,\n",
        "                        label=\"Protecci√≥n Facial\"\n",
        "                    )\n",
        "                    crouch_threshold = gr.Slider(\n",
        "                        minimum=0.1, maximum=0.5, value=0.25, step=0.01,\n",
        "                        label=\"Postura Agachada\"\n",
        "                    )\n",
        "                    movement_history = gr.Slider(\n",
        "                        minimum=5, maximum=50, value=15, step=5,\n",
        "                        label=\"Historial de Movimiento\"\n",
        "                    )\n",
        "\n",
        "                mostrar_interaccion = gr.Radio(\n",
        "                    choices=[\"Ambos\", \"Agresor\", \"V√≠ctima\"],\n",
        "                    value=\"Ambos\",\n",
        "                    label=\"üì∏ Mostrar en Resultados\"\n",
        "                )\n",
        "\n",
        "                btn_analizar = gr.Button(\n",
        "                    \"üöÄ Iniciar An√°lisis\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### üìä Resultados\")\n",
        "\n",
        "                with gr.Tab(\"Video Procesado\"):\n",
        "                    output_video = gr.Video(label=\"Video Analizado\")\n",
        "\n",
        "                with gr.Tab(\"Captura\"):\n",
        "                    output_screenshot = gr.Image(label=\"Primera Interacci√≥n Detectada\")\n",
        "\n",
        "                with gr.Tab(\"Reporte\"):\n",
        "                    output_text = gr.Markdown()\n",
        "\n",
        "        # Conectar eventos\n",
        "        btn_analizar.click(\n",
        "            fn=identificar_victima_y_agresor,\n",
        "            inputs=[\n",
        "                video_input, youtube_url,\n",
        "                aggressor_threshold, victim_threshold,\n",
        "                proximity_threshold, min_duration_frames,\n",
        "                dist_threshold_px, speed_threshold,\n",
        "                fist_distance_threshold, face_cover_threshold,\n",
        "                crouch_threshold, movement_history,\n",
        "                mostrar_interaccion\n",
        "            ],\n",
        "            outputs=[output_video, output_screenshot, output_text]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### ‚ÑπÔ∏è Instrucciones:\n",
        "        1. Sube un video o ingresa una URL de YouTube\n",
        "        2. Ajusta los par√°metros seg√∫n necesites\n",
        "        3. Haz clic en 'Iniciar An√°lisis'\n",
        "        4. Revisa los resultados en las pesta√±as\n",
        "\n",
        "        **Nota:** El procesamiento puede tomar varios minutos dependiendo de la duraci√≥n del video.\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# ==========================================================================================================\n",
        "# LANZAR APLICACI√ìN\n",
        "# ==========================================================================================================\n",
        "# ==========================================================================================================\n",
        "# LANZAR EN COLAB - VERSI√ìN OPTIMIZADA\n",
        "# ==========================================================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Iniciando aplicaci√≥n...\")\n",
        "\n",
        "    # Crear interfaz\n",
        "    demo = create_interface()\n",
        "\n",
        "    # Detectar si estamos en Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    if IN_COLAB:\n",
        "        print(\"üèÅ Entorno: Google Colab detectado\")\n",
        "        # En Colab, usar share=True para obtener URL p√∫blica\n",
        "        demo.launch(share=True, debug=False)\n",
        "    else:\n",
        "        print(\"üèÅ Entorno: Local/Jupyter detectado\")\n",
        "        # En local, buscar puerto disponible\n",
        "        import socket\n",
        "        from contextlib import closing\n",
        "\n",
        "        def find_free_port():\n",
        "            with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "                s.bind(('', 0))\n",
        "                s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "                return s.getsockname()[1]\n",
        "\n",
        "        free_port = find_free_port()\n",
        "        print(f\"‚úÖ Usando puerto: {free_port}\")\n",
        "\n",
        "        demo.launch(\n",
        "            server_name=\"0.0.0.0\",\n",
        "            server_port=free_port,\n",
        "            share=False,\n",
        "            inbrowser=True,\n",
        "            show_error=True\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a3szTSPbsJsg",
        "outputId": "06e3baea-471f-4241-d336-69ccd1aa8d8b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando Identificador V√≠ctima y Agresor...\n",
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c5bdbdae6dfd730e39.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c5bdbdae6dfd730e39.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üöÄ CARGANDO MODELOS\n",
            "================================================================================\n",
            "Cargando modelo YOLO...\n",
            "‚úì Modelo YOLO cargado\n",
            "‚úì Clasificador de violencia cargado\n",
            "================================================================================\n",
            "\n",
            "Iniciando aplicaci√≥n...\n",
            "üèÅ Entorno: Google Colab detectado\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://311e42a44f77a09641.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://311e42a44f77a09641.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo que procesa videos completos"
      ],
      "metadata": {
        "id": "UyVQOnZ2Ks44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"violence_pipeline_mejorado.py - Sistema completo de detecci√≥n de violencia\n",
        "Incluye interfaz Gradio funcional con an√°lisis ML y detecci√≥n de poses\n",
        "\"\"\"\n",
        "\n",
        "import gradio as gr\n",
        "import yt_dlp\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "import logging\n",
        "\n",
        "# Configurar logger\n",
        "logging.basicConfig(\n",
        "    filename='app.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==========================================================================================================\n",
        "# PAR√ÅMETROS GLOBALES CONFIGURABLES (ajustados seg√∫n recomendaciones)\n",
        "# ==========================================================================================================\n",
        "GLOBAL_PARAMS = {\n",
        "    \"AGGRESSOR_THRESHOLD\": 2.8,\n",
        "    \"VICTIM_THRESHOLD\": 1.5,\n",
        "    \"PROXIMITY_THRESHOLD\": 140,\n",
        "    \"MIN_DURATION_FRAMES\": 15,\n",
        "    \"dist_threshold_px\": 85,\n",
        "    \"speed_threshold\": 0.07,\n",
        "    \"FIST_DISTANCE_THRESHOLD\": 0.12,\n",
        "    \"FACE_COVER_THRESHOLD\": 0.10,\n",
        "    \"CROUCH_THRESHOLD\": 0.18,\n",
        "    \"disappearance_threshold\": 25,\n",
        "    \"movement_history\": 15,\n",
        "    \"SMOOTH_WINDOW\": 12,\n",
        "}\n",
        "\n",
        "def update_global_params(new_params):\n",
        "    \"\"\"Actualiza los par√°metros globales\"\"\"\n",
        "    GLOBAL_PARAMS.update(new_params)\n",
        "\n",
        "# ==========================================================================================================\n",
        "# UTILIDADES MEJORADAS\n",
        "# ==========================================================================================================\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    try:\n",
        "        a, b, c = np.array(a, dtype=float), np.array(b, dtype=float), np.array(c, dtype=float)\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        denom = (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)\n",
        "        cosine_angle = np.dot(ba, bc) / denom\n",
        "        cosine_angle = np.clip(cosine_angle, -1, 1)\n",
        "        return np.degrees(np.arccos(cosine_angle))\n",
        "    except Exception:\n",
        "        return 180.0\n",
        "\n",
        "def safe_kp(kp_list, idx):\n",
        "    \"\"\"Devuelve (x,y,conf) o [0,0,0]\"\"\"\n",
        "    try:\n",
        "        return kp_list[idx]\n",
        "    except Exception:\n",
        "        return np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "def detect_punch_direction(wrist, prev_wrist, target_torso, frame_scale=1.0):\n",
        "    \"\"\"Retorna intensidad estimada de golpe dirigido hacia target_torso\"\"\"\n",
        "    try:\n",
        "        if wrist[2] < 0.3 or prev_wrist[2] < 0.3:\n",
        "            return 0.0\n",
        "        v = np.array(wrist[:2]) - np.array(prev_wrist[:2])\n",
        "        speed = np.linalg.norm(v)\n",
        "        to_target = np.array(target_torso) - np.array(wrist[:2])\n",
        "        if np.linalg.norm(to_target) == 0:\n",
        "            return 0.0\n",
        "        angle = calculate_angle([0, 0], v, to_target)\n",
        "        if speed > 6 and angle < 35:\n",
        "            return (speed / (frame_scale + 1e-6)) * 0.12\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def detect_hook_punch(elbow, prev_elbow):\n",
        "    \"\"\"Detecta movimiento circular en codo (gancho)\"\"\"\n",
        "    try:\n",
        "        if elbow[2] < 0.3 or prev_elbow[2] < 0.3:\n",
        "            return 0.0\n",
        "        v = np.array(elbow[:2]) - np.array(prev_elbow[:2])\n",
        "        angular_speed = np.linalg.norm(v)\n",
        "        return 1.2 if angular_speed > 4 else 0.0\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "def detect_push_posture(keypoints, prev_keypoints, img_height):\n",
        "    NOSE, L_HIP, R_HIP = 0, 11, 12\n",
        "    try:\n",
        "        if keypoints[NOSE][2] < 0.3 or keypoints[L_HIP][2] < 0.3 or keypoints[R_HIP][2] < 0.3:\n",
        "            return 0.0\n",
        "        torso_now = (keypoints[L_HIP][:2] + keypoints[R_HIP][:2]) / 2\n",
        "        nose = np.array(keypoints[NOSE][:2])\n",
        "        lean_dist = np.linalg.norm(nose - torso_now)\n",
        "        if prev_keypoints is None or prev_keypoints[NOSE][2] < 0.3:\n",
        "            return 0.0\n",
        "        prev_nose = prev_keypoints[NOSE][:2]\n",
        "        lean_speed = np.linalg.norm(nose - prev_nose)\n",
        "        if lean_dist > 0.06 * img_height and lean_speed > 2:\n",
        "            return lean_speed * 0.3\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def compute_acceleration(history_deque):\n",
        "    \"\"\"Calcula aceleraci√≥n desde historial de posiciones\"\"\"\n",
        "    try:\n",
        "        if len(history_deque) < 3:\n",
        "            return 0.0\n",
        "        p1 = np.array(history_deque[-3])\n",
        "        p2 = np.array(history_deque[-2])\n",
        "        p3 = np.array(history_deque[-1])\n",
        "        v1 = np.linalg.norm(p2 - p1)\n",
        "        v2 = np.linalg.norm(p3 - p2)\n",
        "        return v2 - v1\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "def hand_to_body_distance(hand, target_box):\n",
        "    try:\n",
        "        hx, hy = hand[:2]\n",
        "        tx1, ty1, w, h = target_box\n",
        "        cx, cy = tx1 + w / 2.0, ty1 + h / 2.0\n",
        "        return np.linalg.norm([hx - cx, hy - cy])\n",
        "    except Exception:\n",
        "        return float('inf')\n",
        "\n",
        "def detect_collapse(keypoints, prev_keypoints):\n",
        "    \"\"\"Detecta ca√≠da brusca (knockdown)\"\"\"\n",
        "    try:\n",
        "        L_HIP, R_HIP = 11, 12\n",
        "        if keypoints[L_HIP][2] < 0.3 or prev_keypoints is None or prev_keypoints[L_HIP][2] < 0.3:\n",
        "            return 0.0\n",
        "        hip_now = (keypoints[L_HIP][:2] + keypoints[R_HIP][:2]) / 2\n",
        "        hip_prev = (prev_keypoints[L_HIP][:2] + prev_keypoints[R_HIP][:2]) / 2\n",
        "        dy = hip_prev[1] - hip_now[1]\n",
        "        if dy < -20:\n",
        "            return abs(dy) * 0.05\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "# ==========================================================================================================\n",
        "# DETECTORES DE POSTURAS MEJORADOS\n",
        "# ==========================================================================================================\n",
        "\n",
        "def detect_offensive_postures(keypoints, box, img_height, prev_keypoints=None, target_boxes=None):\n",
        "    \"\"\"Detecci√≥n mejorada de posturas ofensivas\"\"\"\n",
        "    features = {'arms_raised': 0.0, 'fists_closed': 0.0, 'forward_lean': 0.0,\n",
        "                'offensive_gesture': 0.0, 'punch_direction': 0.0, 'hook': 0.0, 'push': 0.0,\n",
        "                'total_score': 0.0}\n",
        "\n",
        "    CONFIDENCE_THRESHOLD = 0.3\n",
        "    FIST_DISTANCE_THRESHOLD = GLOBAL_PARAMS[\"FIST_DISTANCE_THRESHOLD\"]\n",
        "    NOSE, L_SHOULDER, R_SHOULDER = 0, 5, 6\n",
        "    L_ELBOW, R_ELBOW, L_WRIST, R_WRIST = 7, 8, 9, 10\n",
        "\n",
        "    try:\n",
        "        # Brazos levantados\n",
        "        if keypoints[L_WRIST][2] > CONFIDENCE_THRESHOLD and keypoints[L_SHOULDER][2] > CONFIDENCE_THRESHOLD:\n",
        "            if keypoints[L_WRIST][1] < keypoints[L_SHOULDER][1]:\n",
        "                features['arms_raised'] += 1.0\n",
        "        if keypoints[R_WRIST][2] > CONFIDENCE_THRESHOLD and keypoints[R_SHOULDER][2] > CONFIDENCE_THRESHOLD:\n",
        "            if keypoints[R_WRIST][1] < keypoints[R_SHOULDER][1]:\n",
        "                features['arms_raised'] += 1.0\n",
        "\n",
        "        # Pu√±os cerrados\n",
        "        if keypoints[L_WRIST][2] > CONFIDENCE_THRESHOLD and keypoints[L_ELBOW][2] > CONFIDENCE_THRESHOLD:\n",
        "            dist_le = np.linalg.norm(keypoints[L_WRIST][:2] - keypoints[L_ELBOW][:2])\n",
        "            if dist_le < FIST_DISTANCE_THRESHOLD * img_height:\n",
        "                features['fists_closed'] += 1.5\n",
        "        if keypoints[R_WRIST][2] > CONFIDENCE_THRESHOLD and keypoints[R_ELBOW][2] > CONFIDENCE_THRESHOLD:\n",
        "            dist_re = np.linalg.norm(keypoints[R_WRIST][:2] - keypoints[R_ELBOW][:2])\n",
        "            if dist_re < FIST_DISTANCE_THRESHOLD * img_height:\n",
        "                features['fists_closed'] += 1.5\n",
        "\n",
        "        # Inclinaci√≥n hacia adelante\n",
        "        if all(keypoints[i][2] > CONFIDENCE_THRESHOLD for i in [NOSE, L_SHOULDER, R_SHOULDER]):\n",
        "            shoulder_center = (keypoints[L_SHOULDER][:2] + keypoints[R_SHOULDER][:2]) / 2.0\n",
        "            if keypoints[NOSE][0] > shoulder_center[0]:\n",
        "                features['forward_lean'] += 1.0\n",
        "\n",
        "        # Gestos ofensivos por √°ngulos\n",
        "        for shoulder, elbow, wrist in [(L_SHOULDER, L_ELBOW, L_WRIST), (R_SHOULDER, R_ELBOW, R_WRIST)]:\n",
        "            if all(keypoints[i][2] > CONFIDENCE_THRESHOLD for i in [shoulder, elbow, wrist]):\n",
        "                angle = calculate_angle(keypoints[shoulder][:2], keypoints[elbow][:2], keypoints[wrist][:2])\n",
        "                if 150 < angle < 180:\n",
        "                    features['offensive_gesture'] += 2.0\n",
        "\n",
        "        # Detecci√≥n de golpes dirigidos\n",
        "        if prev_keypoints is not None and target_boxes is not None:\n",
        "            for wrist_idx in [L_WRIST, R_WRIST]:\n",
        "                prev_w = prev_keypoints[wrist_idx]\n",
        "                curr_w = keypoints[wrist_idx]\n",
        "                if curr_w[2] > CONFIDENCE_THRESHOLD and prev_w[2] > CONFIDENCE_THRESHOLD:\n",
        "                    for tb in target_boxes:\n",
        "                        center_tb = (tb[0] + tb[2] / 2.0, tb[1] + tb[3] / 2.0)\n",
        "                        punch_score = detect_punch_direction(curr_w, prev_w, center_tb, frame_scale=img_height)\n",
        "                        if punch_score > 0:\n",
        "                            features['punch_direction'] += punch_score\n",
        "\n",
        "                    # Detecci√≥n de gancho\n",
        "                    elbow_idx = L_ELBOW if wrist_idx == L_WRIST else R_ELBOW\n",
        "                    prev_el = prev_keypoints[elbow_idx]\n",
        "                    curr_el = keypoints[elbow_idx]\n",
        "                    features['hook'] += detect_hook_punch(curr_el, prev_el)\n",
        "\n",
        "        # Postura de empuje\n",
        "        if prev_keypoints is not None:\n",
        "            features['push'] += detect_push_posture(keypoints, prev_keypoints, img_height)\n",
        "\n",
        "        features['total_score'] = (features['arms_raised'] + features['fists_closed'] +\n",
        "                                   features['forward_lean'] + features['offensive_gesture'] +\n",
        "                                   features['punch_direction'] + features['hook'] + features['push'])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return features\n",
        "\n",
        "def detect_defensive_postures(keypoints, box, img_height, prev_keypoints=None):\n",
        "    \"\"\"Detecci√≥n mejorada de posturas defensivas\"\"\"\n",
        "    features = {'covering_face': 0.0, 'crouching': 0.0, 'defensive_arms': 0.0, 'collapse': 0.0, 'total_score': 0.0}\n",
        "\n",
        "    CONFIDENCE_THRESHOLD = 0.3\n",
        "    FACE_COVER_THRESHOLD = GLOBAL_PARAMS[\"FACE_COVER_THRESHOLD\"]\n",
        "    CROUCH_THRESHOLD = GLOBAL_PARAMS[\"CROUCH_THRESHOLD\"]\n",
        "    NOSE, L_EYE, R_EYE, L_EAR, R_EAR = 0, 1, 2, 3, 4\n",
        "    L_SHOULDER, R_SHOULDER, L_ELBOW, R_ELBOW = 5, 6, 7, 8\n",
        "    L_WRIST, R_WRIST, L_HIP, R_HIP = 9, 10, 11, 12\n",
        "\n",
        "    try:\n",
        "        # Protecci√≥n facial\n",
        "        face_points = [NOSE, L_EYE, R_EYE, L_EAR, R_EAR]\n",
        "        valid_face_points = [keypoints[i][:2] for i in face_points if keypoints[i][2] > CONFIDENCE_THRESHOLD]\n",
        "        if valid_face_points:\n",
        "            face_center = np.mean(valid_face_points, axis=0)\n",
        "            for wrist in [L_WRIST, R_WRIST]:\n",
        "                if keypoints[wrist][2] > CONFIDENCE_THRESHOLD:\n",
        "                    dist = np.linalg.norm(keypoints[wrist][:2] - face_center)\n",
        "                    if dist < FACE_COVER_THRESHOLD * img_height:\n",
        "                        features['covering_face'] += 2.0\n",
        "\n",
        "        # Postura agachada\n",
        "        if keypoints[L_SHOULDER][2] > CONFIDENCE_THRESHOLD and keypoints[L_HIP][2] > CONFIDENCE_THRESHOLD:\n",
        "            upper_body_height = abs(keypoints[L_SHOULDER][1] - keypoints[L_HIP][1])\n",
        "            if upper_body_height < CROUCH_THRESHOLD * img_height:\n",
        "                features['crouching'] += 1.5\n",
        "\n",
        "        # Brazos defensivos\n",
        "        for shoulder, elbow, wrist in [(L_SHOULDER, L_ELBOW, L_WRIST), (R_SHOULDER, R_ELBOW, R_WRIST)]:\n",
        "            if all(keypoints[i][2] > CONFIDENCE_THRESHOLD for i in [shoulder, elbow, wrist]):\n",
        "                angle = calculate_angle(keypoints[shoulder][:2], keypoints[elbow][:2], keypoints[wrist][:2])\n",
        "                if 60 < angle < 120:\n",
        "                    features['defensive_arms'] += 1.0\n",
        "\n",
        "        # Colapso/ca√≠da\n",
        "        if prev_keypoints is not None:\n",
        "            features['collapse'] += detect_collapse(keypoints, prev_keypoints)\n",
        "\n",
        "        features['total_score'] = features['covering_face'] + features['crouching'] + features['defensive_arms'] + features['collapse']\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return features\n",
        "\n",
        "# ==========================================================================================================\n",
        "# SISTEMA DE AN√ÅLISIS MEJORADO - VERSI√ìN SIMPLIFICADA PERO FUNCIONAL\n",
        "# ==========================================================================================================\n",
        "\n",
        "def analyze_interactions_simple(person_boxes, keypoints_list, frame_height):\n",
        "    \"\"\"Versi√≥n simplificada pero funcional del an√°lisis de interacciones\"\"\"\n",
        "    interactions = {'proximity_scores': {}, 'aggression_vectors': {}}\n",
        "\n",
        "    try:\n",
        "        for i, (box_i, kp_i) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            interactions['proximity_scores'][i] = 0.0\n",
        "            interactions['aggression_vectors'][i] = 0.0\n",
        "\n",
        "            center_i = np.array([box_i[0] + box_i[2]/2.0, box_i[1] + box_i[3]/2.0])\n",
        "            offensive_score = detect_offensive_postures(kp_i, box_i, frame_height).get('total_score', 0.0)\n",
        "\n",
        "            for j, (box_j, kp_j) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                center_j = np.array([box_j[0] + box_j[2]/2.0, box_j[1] + box_j[3]/2.0])\n",
        "                distance = np.linalg.norm(center_i - center_j)\n",
        "\n",
        "                # Proximidad\n",
        "                if distance < GLOBAL_PARAMS[\"PROXIMITY_THRESHOLD\"]:\n",
        "                    proximity_score = (GLOBAL_PARAMS[\"PROXIMITY_THRESHOLD\"] - distance) / GLOBAL_PARAMS[\"PROXIMITY_THRESHOLD\"]\n",
        "                    interactions['proximity_scores'][i] += proximity_score\n",
        "\n",
        "                # Agresi√≥n basada en postura ofensiva y proximidad\n",
        "                if offensive_score > 1.0 and distance < 200:\n",
        "                    interactions['aggression_vectors'][i] += offensive_score * 0.5\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en analyze_interactions_simple: {e}\")\n",
        "\n",
        "    return interactions\n",
        "\n",
        "def assign_roles_simple(person_boxes, keypoints_list, frame_height):\n",
        "    \"\"\"Asignaci√≥n simplificada de roles que S√ç funciona\"\"\"\n",
        "    roles = []\n",
        "\n",
        "    try:\n",
        "        if len(person_boxes) < 2:\n",
        "            return roles\n",
        "\n",
        "        interactions = analyze_interactions_simple(person_boxes, keypoints_list, frame_height)\n",
        "\n",
        "        for i, (box, kp) in enumerate(zip(person_boxes, keypoints_list)):\n",
        "            offensive_score = detect_offensive_postures(kp, box, frame_height).get('total_score', 0.0)\n",
        "            defensive_score = detect_defensive_postures(kp, box, frame_height).get('total_score', 0.0)\n",
        "            proximity_score = interactions['proximity_scores'].get(i, 0.0)\n",
        "            aggression_score = interactions['aggression_vectors'].get(i, 0.0)\n",
        "\n",
        "            # Puntuaci√≥n final m√°s simple pero efectiva\n",
        "            final_score = (offensive_score * 1.5 + aggression_score * 1.2 +\n",
        "                          proximity_score * 0.8 - defensive_score * 0.5)\n",
        "\n",
        "            victim_score = (defensive_score * 1.5 + proximity_score * 0.6 -\n",
        "                           offensive_score * 0.3)\n",
        "\n",
        "            if final_score >= GLOBAL_PARAMS[\"AGGRESSOR_THRESHOLD\"]:\n",
        "                roles.append((i, 'agresor', final_score))\n",
        "            elif victim_score >= GLOBAL_PARAMS[\"VICTIM_THRESHOLD\"]:\n",
        "                roles.append((i, 'victima', victim_score))\n",
        "            else:\n",
        "                roles.append((i, 'desconocido', final_score))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en assign_roles_simple: {e}\")\n",
        "\n",
        "    return roles\n",
        "\n",
        "# ==========================================================================================================\n",
        "# PROCESAMIENTO DE VIDEO MEJORADO - VERSI√ìN QUE S√ç FUNCIONA\n",
        "# ==========================================================================================================\n",
        "\n",
        "def process_video_full_analysis(video_input, output_dir='outputs_rwf', max_frames=None, use_advanced=True, yolo_model=None, progress=None, frame_skip=2):\n",
        "    \"\"\"Funci√≥n principal MEJORADA que S√ç detecta agresores y v√≠ctimas\"\"\"\n",
        "    try:\n",
        "        if progress is not None:\n",
        "            progress(0, \"üîÑ Preparando video...\")\n",
        "\n",
        "        # Manejar entrada de video\n",
        "        if isinstance(video_input, str) and os.path.exists(video_input):\n",
        "            video_path = video_input\n",
        "            video_name = os.path.basename(video_input).split('.')[0]\n",
        "        else:\n",
        "            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:\n",
        "                if hasattr(video_input, 'read'):\n",
        "                    temp_file.write(video_input.read())\n",
        "                video_path = temp_file.name\n",
        "            video_name = \"video_temp\"\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return 0, 0, 0, None, 0, 0, [], None, None, None\n",
        "\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS)) if cap.get(cv2.CAP_PROP_FPS) > 0 else 25\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Procesar TODO el video\n",
        "        max_frames = total_frames\n",
        "\n",
        "        # Configurar salida con MP4 para compatibilidad\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        output_path = os.path.join(output_dir, f\"{video_name}_processed.mp4\")\n",
        "        out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (640, 480))\n",
        "\n",
        "        # Variables de seguimiento MEJORADAS\n",
        "        frame_count = 0\n",
        "        processed_frame_idx = 0\n",
        "        unique_aggressors = set()\n",
        "        unique_victims = set()\n",
        "        aggressor_frame_count = 0\n",
        "        victim_frame_count = 0\n",
        "\n",
        "        # Capturas\n",
        "        first_agg_path = first_vic_path = first_both_path = None\n",
        "        screenshot_saved_agg = screenshot_saved_vic = screenshot_saved_both = False\n",
        "\n",
        "        if progress is not None:\n",
        "            progress(0.1, \"üéØ Iniciando an√°lisis de frames...\")\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret or frame_count >= max_frames:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_skip != 0:\n",
        "                frame_count += 1\n",
        "                continue\n",
        "\n",
        "            if progress is not None and frame_count % 10 == 0:\n",
        "                progress(frame_count / max_frames, f\"üìä Procesando frame {frame_count}/{max_frames}\")\n",
        "\n",
        "            # Redimensionar frame\n",
        "            frame_resized = cv2.resize(frame, (640, 480))\n",
        "\n",
        "            # Detectar personas con YOLO\n",
        "            results = yolo_model(frame_resized, verbose=False)\n",
        "\n",
        "            person_boxes = []\n",
        "            keypoints_list = []\n",
        "            current_aggressors = []\n",
        "            current_victims = []\n",
        "\n",
        "            if results and len(results) > 0:\n",
        "                result = results[0]\n",
        "                if result.boxes is not None and result.keypoints is not None:\n",
        "                    for i, box in enumerate(result.boxes):\n",
        "                        if int(box.cls) == 0:  # Solo personas\n",
        "                            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
        "                            conf = box.conf.item()\n",
        "                            kp = result.keypoints.data[i].cpu().numpy() if i < len(result.keypoints.data) else np.zeros((17, 3))\n",
        "\n",
        "                            person_boxes.append((x1, y1, x2 - x1, y2 - y1))\n",
        "                            keypoints_list.append(kp)\n",
        "\n",
        "            # ‚úÖ AN√ÅLISIS MEJORADO: Usar el sistema de detecci√≥n MEJORADO\n",
        "            if len(person_boxes) >= 2:  # Solo analizar si hay al menos 2 personas\n",
        "                roles = assign_roles_simple(person_boxes, keypoints_list, 480)\n",
        "\n",
        "                for person_idx, role, score in roles:\n",
        "                    if role == 'agresor':\n",
        "                        current_aggressors.append(person_idx)\n",
        "                        unique_aggressors.add(person_idx)\n",
        "                        color = (0, 0, 255)  # Rojo\n",
        "                        label = f\"Agresor ({score:.1f})\"\n",
        "                    elif role == 'victima':\n",
        "                        current_victims.append(person_idx)\n",
        "                        unique_victims.add(person_idx)\n",
        "                        color = (255, 0, 0)  # Azul\n",
        "                        label = f\"Victima ({score:.1f})\"\n",
        "                    else:\n",
        "                        color = (0, 255, 0)  # Verde\n",
        "                        label = \"Desconocido\"\n",
        "\n",
        "                    # Dibujar bounding box y etiqueta\n",
        "                    if person_idx < len(person_boxes):\n",
        "                        x, y, w, h = [int(v) for v in person_boxes[person_idx]]\n",
        "                        cv2.rectangle(frame_resized, (x, y), (x + w, y + h), color, 2)\n",
        "                        cv2.putText(frame_resized, label, (x, y - 10),\n",
        "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "            # Actualizar contadores\n",
        "            if current_aggressors:\n",
        "                aggressor_frame_count += 1\n",
        "            if current_victims:\n",
        "                victim_frame_count += 1\n",
        "\n",
        "            # Guardar capturas MEJORADO\n",
        "            if not screenshot_saved_agg and current_aggressors:\n",
        "                first_agg_path = os.path.join(output_dir, f\"{video_name}_primer_agresor.jpg\")\n",
        "                cv2.imwrite(first_agg_path, frame_resized)\n",
        "                screenshot_saved_agg = True\n",
        "                logger.info(f\"‚úÖ Captura de agresor guardada\")\n",
        "\n",
        "            if not screenshot_saved_vic and current_victims:\n",
        "                first_vic_path = os.path.join(output_dir, f\"{video_name}_primera_victima.jpg\")\n",
        "                cv2.imwrite(first_vic_path, frame_resized)\n",
        "                screenshot_saved_vic = True\n",
        "                logger.info(f\"‚úÖ Captura de v√≠ctima guardada\")\n",
        "\n",
        "            if not screenshot_saved_both and current_aggressors and current_victims:\n",
        "                first_both_path = os.path.join(output_dir, f\"{video_name}_primer_ambos.jpg\")\n",
        "                cv2.imwrite(first_both_path, frame_resized)\n",
        "                screenshot_saved_both = True\n",
        "                logger.info(f\"‚úÖ Captura de interacci√≥n guardada\")\n",
        "\n",
        "            # Informaci√≥n en pantalla\n",
        "            info_text = f\"Frame: {frame_count} | Personas: {len(person_boxes)} | Agresores: {len(current_aggressors)} | Victimas: {len(current_victims)}\"\n",
        "            cv2.putText(frame_resized, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "            # Escribir frame procesado\n",
        "            out.write(frame_resized)\n",
        "            frame_count += 1\n",
        "            processed_frame_idx += 1\n",
        "\n",
        "        # Liberar recursos\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Limpiar archivo temporal si se cre√≥\n",
        "        if 'temp_file' in locals():\n",
        "            try:\n",
        "                os.unlink(video_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        logger.info(f\"‚úÖ An√°lisis completado: {len(unique_aggressors)} agresores, {len(unique_victims)} v√≠ctimas\")\n",
        "\n",
        "        if progress is not None:\n",
        "            progress(1.0, \"‚úÖ An√°lisis completado!\")\n",
        "\n",
        "        return (\n",
        "            len(unique_aggressors),\n",
        "            len(unique_victims),\n",
        "            processed_frame_idx,\n",
        "            output_path,\n",
        "            aggressor_frame_count,\n",
        "            victim_frame_count,\n",
        "            [],  # eventos\n",
        "            first_agg_path,\n",
        "            first_vic_path,\n",
        "            first_both_path\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error en process_video_full_analysis: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return 0, 0, 0, None, 0, 0, [], None, None, None\n",
        "\n",
        "# ==========================================================================================================\n",
        "# EXTRACTOR DE FEATURES CORREGIDO\n",
        "# ==========================================================================================================\n",
        "\n",
        "def extract_lightweight_features(video_input, max_samples=10, yolo_model=None, progress=None):\n",
        "    \"\"\"Extrae features b√°sicas para clasificaci√≥n ML - VERSI√ìN CORREGIDA\"\"\"\n",
        "    try:\n",
        "        if isinstance(video_input, str) and os.path.exists(video_input):\n",
        "            video_path = video_input\n",
        "        else:\n",
        "            return np.zeros(6)  # Fallback\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            return np.zeros(6)\n",
        "\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        sample_frames = min(max_samples, total_frames)\n",
        "\n",
        "        # Features b√°sicas pero reales\n",
        "        person_counts = []\n",
        "        motion_values = []\n",
        "\n",
        "        for i in range(sample_frames):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_resized = cv2.resize(frame, (416, 416))\n",
        "\n",
        "            # Contar personas\n",
        "            results = yolo_model(frame_resized, verbose=False, classes=[0])\n",
        "            person_count = len(results[0].boxes) if results else 0\n",
        "            person_counts.append(person_count)\n",
        "\n",
        "            # Movimiento simple (diferencia de frames)\n",
        "            if i > 0:\n",
        "                gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
        "                prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "                diff = cv2.absdiff(gray, prev_gray)\n",
        "                motion_values.append(np.mean(diff))\n",
        "\n",
        "            prev_frame = frame_resized.copy()\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Crear features\n",
        "        features = np.array([\n",
        "            np.mean(person_counts) if person_counts else 0,\n",
        "            np.max(person_counts) if person_counts else 0,\n",
        "            np.std(person_counts) if person_counts else 0,\n",
        "            np.mean(motion_values) if motion_values else 0,\n",
        "            np.max(motion_values) if motion_values else 0,\n",
        "            total_frames / 1000.0  # Duraci√≥n aproximada\n",
        "        ])\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en extract_lightweight_features: {e}\")\n",
        "        return np.zeros(6)\n",
        "\n",
        "# ==========================================================================================================\n",
        "# FUNCI√ìN PRINCIPAL MEJORADA\n",
        "# ==========================================================================================================\n",
        "\n",
        "def identificar_victima_y_agresor(\n",
        "    video_input=None,\n",
        "    youtube_url=\"\",\n",
        "    aggressor_threshold=2.8,\n",
        "    victim_threshold=1.5,\n",
        "    proximity_threshold=140,\n",
        "    min_duration_frames=15,\n",
        "    dist_threshold_px=85,\n",
        "    speed_threshold=0.04,\n",
        "    fist_distance_threshold=0.08,\n",
        "    face_cover_threshold=0.10,\n",
        "    crouch_threshold=0.25,\n",
        "    movement_history=15,\n",
        "    mostrar_interaccion=\"Ambos\",\n",
        "    progress=gr.Progress()\n",
        "):\n",
        "    \"\"\"Funci√≥n principal MEJORADA con detecci√≥n REAL\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Actualizar par√°metros globales con tus valores MEJORADOS\n",
        "        update_global_params({\n",
        "            \"AGGRESSOR_THRESHOLD\": aggressor_threshold,\n",
        "            \"VICTIM_THRESHOLD\": victim_threshold,\n",
        "            \"PROXIMITY_THRESHOLD\": proximity_threshold,\n",
        "            \"MIN_DURATION_FRAMES\": min_duration_frames,\n",
        "            \"dist_threshold_px\": dist_threshold_px,\n",
        "            \"speed_threshold\": speed_threshold,\n",
        "            \"FIST_DISTANCE_THRESHOLD\": fist_distance_threshold,\n",
        "            \"FACE_COVER_THRESHOLD\": face_cover_threshold,\n",
        "            \"CROUCH_THRESHOLD\": crouch_threshold,\n",
        "            \"movement_history\": movement_history,\n",
        "        })\n",
        "\n",
        "        video_path = None\n",
        "        temp_file = None\n",
        "\n",
        "        # Determinar fuente de video\n",
        "        if youtube_url and youtube_url.strip():\n",
        "            progress(0.1, \"üåê Descargando video de YouTube...\")\n",
        "            video_path = descargar_youtube(youtube_url, progress)\n",
        "            temp_file = video_path\n",
        "        elif video_input is not None:\n",
        "            video_path = video_input\n",
        "        else:\n",
        "            raise gr.Error(\"‚ùå Por favor, sube un video o ingresa una URL de YouTube\")\n",
        "\n",
        "        progress(0.3, \"üéØ Iniciando an√°lisis avanzado del video...\")\n",
        "\n",
        "        # ‚úÖ CORRECCI√ìN: Procesar TODO el video\n",
        "        (\n",
        "            num_agresores,\n",
        "            num_victimas,\n",
        "            frames_procesados,\n",
        "            video_procesado_path,\n",
        "            frames_con_agresor,\n",
        "            frames_con_victima,\n",
        "            eventos,\n",
        "            img_agresor,\n",
        "            img_victima,\n",
        "            img_ambos\n",
        "        ) = process_video_full_analysis(\n",
        "            video_path,\n",
        "            output_dir='outputs_rwf',\n",
        "            max_frames=None,  # ‚úÖ Procesar TODO el video\n",
        "            use_advanced=True,\n",
        "            yolo_model=yolo_model,\n",
        "            progress=progress,\n",
        "            frame_skip=2  # Saltar un frame\n",
        "        )\n",
        "\n",
        "        progress(0.9, \"üìä Generando resultados...\")\n",
        "\n",
        "        # Seleccionar imagen para mostrar\n",
        "        img_mostrar = None\n",
        "        mensaje_captura = \"\"\n",
        "\n",
        "        if mostrar_interaccion == \"Agresor\" and img_agresor and os.path.exists(img_agresor):\n",
        "            img_mostrar = img_agresor\n",
        "            mensaje_captura = \"üî¥ Primera detecci√≥n de AGRESOR\"\n",
        "        elif mostrar_interaccion == \"V√≠ctima\" and img_victima and os.path.exists(img_victima):\n",
        "            img_mostrar = img_victima\n",
        "            mensaje_captura = \"üîµ Primera detecci√≥n de V√çCTIMA\"\n",
        "        elif mostrar_interaccion == \"Ambos\" and img_ambos and os.path.exists(img_ambos):\n",
        "            img_mostrar = img_ambos\n",
        "            mensaje_captura = \"üë• Primera interacci√≥n AGRESOR + V√çCTIMA\"\n",
        "        else:\n",
        "            mensaje_captura = \"‚ö† No se capturaron interacciones del tipo seleccionado\"\n",
        "\n",
        "        # ‚úÖ REPORTE MEJORADO con informaci√≥n REAL\n",
        "        violencia_detectada = (frames_con_agresor >= min_duration_frames and\n",
        "                              frames_con_victima >= min_duration_frames)\n",
        "\n",
        "        reporte = f\"\"\"\n",
        "## üìä Resultados del An√°lisis\n",
        "\n",
        "### üë• Detecci√≥n de Personas\n",
        "- **Agresores detectados:** {num_agresores}\n",
        "- **V√≠ctimas detectadas:** {num_victimas}\n",
        "- **Frames procesados:** {frames_procesados}\n",
        "\n",
        "### üéØ Actividad Detectada\n",
        "- **Frames con agresor:** {frames_con_agresor}\n",
        "- **Frames con v√≠ctima:** {frames_con_victima}\n",
        "- **Violencia prolongada:** {'‚úÖ S√ç' if violencia_detectada else '‚ùå NO'}\n",
        "\n",
        "### üì∏ Capturas\n",
        "- **{mensaje_captura}**\n",
        "\n",
        "### ‚öôÔ∏è Configuraci√≥n Usada\n",
        "- **Umbral agresor:** {aggressor_threshold}\n",
        "- **Umbral v√≠ctima:** {victim_threshold}\n",
        "- **Proximidad:** {proximity_threshold}px\n",
        "- **Duraci√≥n m√≠nima:** {min_duration_frames} frames\n",
        "\"\"\"\n",
        "\n",
        "        # Clasificaci√≥n ML MEJORADA (manejo de errores)\n",
        "        if violence_classifier is not None:\n",
        "            try:\n",
        "                features = extract_lightweight_features(video_path, yolo_model=yolo_model)\n",
        "                # ‚úÖ CORRECCI√ìN: Manejar el error de √≠ndice\n",
        "                prediccion = violence_classifier.predict([features])[0]\n",
        "                probabilidad = violence_classifier.predict_proba([features])[0]\n",
        "\n",
        "                # Verificar la forma de las probabilidades\n",
        "                if len(probabilidad) >= 2:\n",
        "                    if prediccion == 1:\n",
        "                        reporte += f\"\\n### üî¥ CLASIFICADOR ML: VIOLENCIA DETECTADA\\n- **Confianza:** {probabilidad[1]:.1%}\"\n",
        "                    else:\n",
        "                        reporte += f\"\\n### ‚úÖ CLASIFICADOR ML: NO VIOLENCIA\\n- **Confianza:** {probabilidad[0]:.1%}\"\n",
        "                elif len(probabilidad) == 1:\n",
        "                    # Caso de clasificador con una sola clase\n",
        "                    confianza = probabilidad[0]\n",
        "                    if prediccion == 1:\n",
        "                        reporte += f\"\\n### üî¥ CLASIFICADOR ML: VIOLENCIA DETECTADA\\n- **Confianza:** {confianza:.1%}\"\n",
        "                    else:\n",
        "                        reporte += f\"\\n### ‚úÖ CLASIFICADOR ML: NO VIOLENCIA\\n- **Confianza:** {confianza:.1%}\"\n",
        "                else:\n",
        "                    reporte += f\"\\n### ‚ö† CLASIFICADOR ML: Resultado - {prediccion}\"\n",
        "\n",
        "            except Exception as e:\n",
        "                reporte += f\"\\n### ‚ö† Clasificaci√≥n ML no disponible\\n- Error: {str(e)}\"\n",
        "        else:\n",
        "            reporte += \"\\n### ‚ÑπÔ∏è Clasificador ML no cargado\"\n",
        "\n",
        "        # Limpiar archivos temporales\n",
        "        if temp_file and os.path.exists(temp_file):\n",
        "            try:\n",
        "                os.remove(temp_file)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        progress(1.0, \"‚úÖ ¬°An√°lisis completado!\")\n",
        "\n",
        "        return video_procesado_path, img_mostrar, reporte\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_msg = f\"Error durante el procesamiento: {str(e)}\"\n",
        "        logger.error(f\"ERROR: {error_msg}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        raise gr.Error(error_msg)\n",
        "\n",
        "# ==========================================================================================================\n",
        "# CARGAR MODELOS\n",
        "# ==========================================================================================================\n",
        "\n",
        "logger.info(\"\\n\" + \"=\"*80)\n",
        "logger.info(\"üöÄ CARGANDO MODELOS\")\n",
        "logger.info(\"=\"*80)\n",
        "\n",
        "logger.info(\"Cargando modelo YOLO...\")\n",
        "try:\n",
        "    yolo_model = YOLO('yolov8n-pose.pt')\n",
        "    logger.info(\"‚úì Modelo YOLO cargado\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"‚ùå Error cargando YOLO: {e}\")\n",
        "    yolo_model = None\n",
        "\n",
        "# Intentar cargar clasificador de violencia\n",
        "model_path = 'violence_classifier.pkl'\n",
        "violence_classifier = None\n",
        "if os.path.exists(model_path):\n",
        "    try:\n",
        "        violence_classifier = joblib.load(model_path)\n",
        "        logger.info(\"‚úì Clasificador de violencia cargado\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ö† Error cargando clasificador: {e}\")\n",
        "else:\n",
        "    logger.warning(\"‚ö† No se encontr√≥ violence_classifier.pkl - El sistema funcionar√° sin clasificaci√≥n ML\")\n",
        "\n",
        "logger.info(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ==========================================================================================================\n",
        "# INTERFAZ GRADIO\n",
        "# ==========================================================================================================\n",
        "\n",
        "def create_interface():\n",
        "    \"\"\"Crea y retorna la interfaz de Gradio\"\"\"\n",
        "\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"Identificador V√≠ctima y Agresor\") as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # üéØ Identificador V√≠ctima y Agresor\n",
        "        ### Sistema de Detecci√≥n de Violencia con IA\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### üìÅ Fuente de Video\")\n",
        "\n",
        "                with gr.Tab(\"Subir Video\"):\n",
        "                    video_input = gr.Video(label=\"Seleccionar video\", sources=[\"upload\"])\n",
        "\n",
        "                with gr.Tab(\"YouTube URL\"):\n",
        "                    youtube_url = gr.Textbox(\n",
        "                        label=\"URL de YouTube\",\n",
        "                        placeholder=\"https://www.youtube.com/watch?v=...\",\n",
        "                        lines=1\n",
        "                    )\n",
        "\n",
        "                gr.Markdown(\"### ‚öôÔ∏è Par√°metros de Detecci√≥n\")\n",
        "\n",
        "                with gr.Accordion(\"Par√°metros Principales\", open=True):\n",
        "                    aggressor_threshold = gr.Slider(\n",
        "                        minimum=0.5, maximum=5.0, value=2.8, step=0.1,\n",
        "                        label=\"Umbral Agresor\"\n",
        "                    )\n",
        "                    victim_threshold = gr.Slider(\n",
        "                        minimum=0.5, maximum=5.0, value=1.5, step=0.1,\n",
        "                        label=\"Umbral V√≠ctima\"\n",
        "                    )\n",
        "                    proximity_threshold = gr.Slider(\n",
        "                        minimum=50, maximum=300, value=140, step=10,\n",
        "                        label=\"Umbral de Proximidad (px)\"\n",
        "                    )\n",
        "                    min_duration_frames = gr.Slider(\n",
        "                        minimum=5, maximum=100, value=15, step=5,\n",
        "                        label=\"Duraci√≥n M√≠nima (frames)\"\n",
        "                    )\n",
        "\n",
        "                with gr.Accordion(\"Par√°metros Avanzados\", open=False):\n",
        "                    dist_threshold_px = gr.Slider(\n",
        "                        minimum=20, maximum=200, value=85, step=5,\n",
        "                        label=\"Distancia para Golpes (px)\"\n",
        "                    )\n",
        "                    speed_threshold = gr.Slider(\n",
        "                        minimum=0.01, maximum=0.2, value=0.04, step=0.01,\n",
        "                        label=\"Umbral de Velocidad\"\n",
        "                    )\n",
        "                    fist_distance_threshold = gr.Slider(\n",
        "                        minimum=0.05, maximum=0.3, value=0.08, step=0.01,\n",
        "                        label=\"Detecci√≥n de Pu√±os\"\n",
        "                    )\n",
        "                    face_cover_threshold = gr.Slider(\n",
        "                        minimum=0.05, maximum=0.3, value=0.10, step=0.01,\n",
        "                        label=\"Protecci√≥n Facial\"\n",
        "                    )\n",
        "                    crouch_threshold = gr.Slider(\n",
        "                        minimum=0.1, maximum=0.5, value=0.25, step=0.01,\n",
        "                        label=\"Postura Agachada\"\n",
        "                    )\n",
        "                    movement_history = gr.Slider(\n",
        "                        minimum=5, maximum=50, value=15, step=5,\n",
        "                        label=\"Historial de Movimiento\"\n",
        "                    )\n",
        "\n",
        "                mostrar_interaccion = gr.Radio(\n",
        "                    choices=[\"Ambos\", \"Agresor\", \"V√≠ctima\"],\n",
        "                    value=\"Ambos\",\n",
        "                    label=\"üì∏ Mostrar en Resultados\"\n",
        "                )\n",
        "\n",
        "                btn_analizar = gr.Button(\n",
        "                    \"üöÄ Iniciar An√°lisis\",\n",
        "                    variant=\"primary\",\n",
        "                    size=\"lg\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### üìä Resultados\")\n",
        "\n",
        "                with gr.Tab(\"Video Procesado\"):\n",
        "                    output_video = gr.Video(label=\"Video Analizado\")\n",
        "\n",
        "                with gr.Tab(\"Captura\"):\n",
        "                    output_screenshot = gr.Image(label=\"Primera Interacci√≥n Detectada\")\n",
        "\n",
        "                with gr.Tab(\"Reporte\"):\n",
        "                    output_text = gr.Markdown()\n",
        "\n",
        "        # Conectar eventos\n",
        "        btn_analizar.click(\n",
        "            fn=identificar_victima_y_agresor,\n",
        "            inputs=[\n",
        "                video_input, youtube_url,\n",
        "                aggressor_threshold, victim_threshold,\n",
        "                proximity_threshold, min_duration_frames,\n",
        "                dist_threshold_px, speed_threshold,\n",
        "                fist_distance_threshold, face_cover_threshold,\n",
        "                crouch_threshold, movement_history,\n",
        "                mostrar_interaccion\n",
        "            ],\n",
        "            outputs=[output_video, output_screenshot, output_text]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### ‚ÑπÔ∏è Instrucciones:\n",
        "        1. Sube un video o ingresa una URL de YouTube\n",
        "        2. Ajusta los par√°metros seg√∫n necesites\n",
        "        3. Haz clic en 'Iniciar An√°lisis'\n",
        "        4. Revisa los resultados en las pesta√±as\n",
        "\n",
        "        **Nota:** El procesamiento puede tomar varios minutos dependiendo de la duraci√≥n del video.\n",
        "        \"\"\")\n",
        "\n",
        "    return demo\n",
        "\n",
        "# ==========================================================================================================\n",
        "# LANZAR APLICACI√ìN\n",
        "# ==========================================================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Iniciando aplicaci√≥n...\")\n",
        "\n",
        "    # Crear interfaz\n",
        "    demo = create_interface()\n",
        "\n",
        "    # Detectar si estamos en Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    if IN_COLAB:\n",
        "        logger.info(\"üèÅ Entorno: Google Colab detectado\")\n",
        "        demo.launch(share=True, debug=False)\n",
        "    else:\n",
        "        logger.info(\"üèÅ Entorno: Local/Jupyter detectado\")\n",
        "        import socket\n",
        "        from contextlib import closing\n",
        "\n",
        "        def find_free_port():\n",
        "            with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "                s.bind(('', 0))\n",
        "                s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
        "                return s.getsockname()[1]\n",
        "\n",
        "        free_port = find_free_port()\n",
        "        logger.info(f\"‚úÖ Usando puerto: {free_port}\")\n",
        "\n",
        "        demo.launch(\n",
        "            server_name=\"0.0.0.0\",\n",
        "            server_port=free_port,\n",
        "            share=False,\n",
        "            inbrowser=True,\n",
        "            show_error=True\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "6vLerCTiKw-Z",
        "outputId": "8ab1f34c-eddd-44f9-e847-0aff4ee5241a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1df560d07b1147bbb5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1df560d07b1147bbb5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mejorar entrenamiento de clasificador"
      ],
      "metadata": {
        "id": "f6gnebnrkdhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================================================\n",
        "# IDENTIFICADOR V√çCTIMA Y AGRESOR - SISTEMA DE DETECCI√ìN DE VIOLENCIA F√çSICA\n",
        "# ==========================================================================================================\n",
        "# Fecha: 12 de noviembre de 2025\n",
        "# Pa√≠s: Chile\n",
        "# Versi√≥n: 1.2 - Sampleo de frames configurable globalmente\n",
        "# ==========================================================================================================\n",
        "#\n",
        "# üéØ CONFIGURACI√ìN R√ÅPIDA DE SAMPLEO DE FRAMES:\n",
        "#\n",
        "# Para cambiar el n√∫mero de frames procesados, modifica GLOBAL_PARAMS o usa:\n",
        "#   configurar_sampleo_frames(training=40, prefilter=12, default=30)\n",
        "#\n",
        "# Presets recomendados:\n",
        "#   ‚ö° VELOCIDAD:    configurar_sampleo_frames(15, 8, 12)   ‚Üí ~25 min entrenamiento\n",
        "#   ‚öñÔ∏è BALANCE:      configurar_sampleo_frames(30, 12, 25)  ‚Üí ~50 min entrenamiento\n",
        "#   üéØ PRECISI√ìN:    configurar_sampleo_frames(50, 15, 40)  ‚Üí ~85 min entrenamiento\n",
        "#   üî¨ M√ÅXIMO:       configurar_sampleo_frames(100, 20, 80) ‚Üí ~170 min entrenamiento\n",
        "#\n",
        "# ==========================================================================================================\n",
        "\n",
        "# 1. Instalaci√≥n de dependencias\n",
        "!pip install -q datasets huggingface_hub ultralytics tensorflow tensorflow_hub opencv-python tqdm scikit-learn matplotlib torch torchvision deep-sort-realtime yt_dlp joblib\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "from collections import defaultdict, deque\n",
        "from ultralytics import YOLO\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import gradio as gr\n",
        "import yt_dlp\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "print(\"‚úì Dependencias instaladas correctamente\")\n",
        "\n",
        "# ==========================================================================================================\n",
        "# CARGAR MODELO YOLO UNA VEZ\n",
        "# ==========================================================================================================\n",
        "print(\"Cargando modelo YOLO...\")\n",
        "try:\n",
        "    yolo_model = YOLO('yolov8n-pose.pt')\n",
        "    print(\"‚úì Modelo YOLO cargado correctamente\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al cargar YOLO: {e}\")\n",
        "    yolo_model = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUMJH5ToqJtU",
        "outputId": "fb393c1b-4b3d-4252-fbfc-0e5d3d441980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "‚úì Dependencias instaladas correctamente\n",
            "Cargando modelo YOLO...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.5MB 90.9MB/s 0.1s\n",
            "‚úì Modelo YOLO cargado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Busqueda de hiperparametros (Queda pendiente l√≠mite RAM)"
      ],
      "metadata": {
        "id": "zzbByItiwlgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# OPTIMIZACI√ìN DE PAR√ÅMETROS - RANDOM SEARCH DISTRIBUIDA\n",
        "# ============================================================================\n",
        "\n",
        "import itertools, random, json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "def random_search_parameter_optimization(dataset, machine_id=1, total_combinations=25, seed_base=42):\n",
        "    \"\"\"\n",
        "    Ejecuta una b√∫squeda aleatoria de par√°metros en paralelo (dividida por machine_id).\n",
        "\n",
        "    Args:\n",
        "        dataset: Dataset de entrada (por ejemplo, dataset['train'])\n",
        "        machine_id: 1 o 2 (para separar combinaciones entre PCs)\n",
        "        total_combinations: cantidad de configuraciones por m√°quina\n",
        "        seed_base: base para la semilla aleatoria\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------\n",
        "    # 1. Definir rangos de par√°metros\n",
        "    # ------------------------------\n",
        "    param_space = {\n",
        "        \"AGGRESSOR_THRESHOLD\": (2.5, 5.0),\n",
        "        \"VICTIM_THRESHOLD\": (1.5, 4.0),\n",
        "        \"PROXIMITY_THRESHOLD\": (100, 200),\n",
        "        \"disappearance_threshold\": (10, 40),\n",
        "        \"movement_history\": (10, 25),\n",
        "        \"speed_threshold\": (0.03, 0.1),\n",
        "        \"dist_threshold_px\": (60, 120),\n",
        "    }\n",
        "\n",
        "    # ------------------------------\n",
        "    # 2. Generar combinaciones aleatorias\n",
        "    # ------------------------------\n",
        "    seed = seed_base + machine_id\n",
        "    random.seed(seed)\n",
        "    param_combinations = []\n",
        "\n",
        "    for _ in range(total_combinations):\n",
        "        combo = {k: round(random.uniform(v[0], v[1]), 3) for k, v in param_space.items()}\n",
        "        param_combinations.append(combo)\n",
        "\n",
        "    # ------------------------------\n",
        "    # 3. Ejecutar pruebas\n",
        "    # ------------------------------\n",
        "    results = []\n",
        "\n",
        "    for idx, params in enumerate(param_combinations, start=1):\n",
        "        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] \"\n",
        "              f\"Machine {machine_id} - Test {idx}/{total_combinations}\")\n",
        "        print(\"Params:\", params)\n",
        "\n",
        "        # Ejecutar con par√°metros actuales\n",
        "        try:\n",
        "            aggressors, victims, frames, _ = process_video_rwf(\n",
        "                random.choice(dataset['train']),  # usa un video aleatorio para test r√°pido\n",
        "                max_frames=100,\n",
        "                disappearance_threshold=int(params[\"disappearance_threshold\"]),\n",
        "                movement_history=int(params[\"movement_history\"]),\n",
        "                use_advanced=True\n",
        "            )\n",
        "\n",
        "            # Calcular m√©tricas\n",
        "            has_interaction = aggressors > 0 and victims > 0\n",
        "            detection_density = (aggressors + victims) / max(frames, 1)\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                [1 if has_interaction else 0], [1 if detection_density > 0.01 else 0],\n",
        "                average='binary', pos_label=1, zero_division=0\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                **params,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall,\n",
        "                \"f1\": f1,\n",
        "                \"detection_density\": detection_density,\n",
        "                \"has_interaction\": has_interaction,\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error en combinaci√≥n {idx}: {e}\")\n",
        "            results.append({**params, \"precision\": 0, \"recall\": 0, \"f1\": 0, \"error\": str(e)})\n",
        "\n",
        "    # ------------------------------\n",
        "    # 4. Guardar resultados\n",
        "    # ------------------------------\n",
        "    df_results = pd.DataFrame(results)\n",
        "    out_path = f\"param_opt_results_pc{machine_id}.csv\"\n",
        "    df_results.to_csv(out_path, index=False)\n",
        "    print(f\"\\n‚úÖ Resultados guardados en: {out_path}\")\n",
        "\n",
        "    # Mostrar top 5 por recall\n",
        "    df_top = df_results.sort_values(by=\"recall\", ascending=False).head(5)\n",
        "    print(\"\\n=== TOP 5 COMBINACIONES POR RECALL ===\")\n",
        "    print(df_top[[\"recall\", \"f1\", \"precision\"] + list(param_space.keys())])\n",
        "\n",
        "    return df_results"
      ],
      "metadata": {
        "id": "9gB0Mne9wnoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CAMILAAAAAA\n",
        "results_pc1 = random_search_parameter_optimization(machine_id=1, total_combinations=25)"
      ],
      "metadata": {
        "id": "aWyiP_EbwzGt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491,
          "referenced_widgets": [
            "a9b1880592d8438884d8c06d91b81459",
            "87d3762c80b3498db5c1e14643386083",
            "fb004fac67274d54bbfba4af1973860e",
            "1ec05ee98add4e4193cbec608daf4623",
            "67fad834604f4eeaaac3342c7a01cafd",
            "31f122008a9d4c8c87a30e94704f5ba6",
            "d5e41ae3ce384798bc2218b939fa0e4d",
            "371e04ffc8314698a208f7f3ff33f4fa",
            "5405c38b04804837809f516f2d86febd",
            "57a8b9adf6ab4c86816d87f8cbf7e117",
            "faffa4a76e8f42b29823841ed3be1071",
            "d9997b075cf74caaa7981c25218c6069",
            "457556bf1a76490d90430096a00c94d7",
            "2ff48a574a6145a48297950fec673154",
            "e65eeaff1e59403089e263e928cf32f7",
            "e6d74e8d2a5547b6879aac2146d21821",
            "854452966377411c97b4183fad289f8a",
            "e109c3915eaf44c7b5df457db6fbd64d",
            "164a7c8a4bcd46d18f1f64ca567f5869",
            "25ad4ee42fb9401788ff0d74bfc83e86",
            "dad3c52ad73b42f0bacd9544e1411ce7",
            "314efd0a7b554cc1ad76313455ddd614"
          ]
        },
        "outputId": "88fdb7b8-0f89-4a35-8745-441b96940ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Cargando dataset local desde 'rwf2000_cached'...\n",
            "‚ö†Ô∏è No se pudo cargar desde 'rwf2000_cached', se descargar√° nuevamente. Error: No such files: '/content/rwf2000_cached/train/dataset_info.json', nor '/content/rwf2000_cached/train/state.json' found. Expected to load a `Dataset` object but provided path is not a `Dataset`.\n",
            "‚¨áÔ∏è Descargando RWF-2000 desde Hugging Face...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9b1880592d8438884d8c06d91b81459"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset descargado. Guardando copia local...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/26 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9997b075cf74caaa7981c25218c6069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1655231141.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# CAMILAAAAAA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_pc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search_parameter_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmachine_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_combinations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2308361521.py\u001b[0m in \u001b[0;36mrandom_search_parameter_optimization\u001b[0;34m(machine_id, total_combinations, seed_base)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Cargar los 10 videos fijos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fixed_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2308361521.py\u001b[0m in \u001b[0;36mload_fixed_videos\u001b[0;34m(seed, local_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Dataset descargado. Guardando copia local...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üíæ Copia guardada en '{local_path}'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36msave_to_disk\u001b[0;34m(self, dataset_dict_path, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"splits\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             dataset.save_to_disk(\n\u001b[0m\u001b[1;32m   1351\u001b[0m                 \u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dict_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 \u001b[0mnum_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_shards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36msave_to_disk\u001b[0;34m(self, dataset_path, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[1;32m   1624\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs_per_job\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_to_disk_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_save_to_disk_single\u001b[0;34m(job_id, shard, fpath, storage_options)\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0m_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpa_table\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m                 \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPBAR_REFRESH_TIME_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_bytes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_stream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/ipc.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}